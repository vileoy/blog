{"pages":[{"title":"About","text":"About me A PhD candidate in theoretical and computational chemistry. Have a wide range of interests, but not proficient. Follow the stars and keep going. About site Log Horizon o(≧v≦)o","link":"/blog/about/index.html"}],"posts":[{"title":"Activation Functions in Artificial NNs","text":"There are different activation functions commonly used in artificial NNs, this post demonstrates an overview of the some activation functions that we are frequently encountered. Activation Functions Overview Activation Functions Essentials Linear compute the logits instead of the class-membership probabilities \\[ \\operatorname{logit}(p(y | \\mathbf{x})) = w_{0} x_{0} + w_{1} x_{1} + \\cdots + w_{m} x_{m} = \\sum_{i=0}^{m} w_{i} x_{i} = \\mathbf{w}^{T} \\mathbf{x} = z \\] Sigmoid for binary classification added at the last (output) layer results in class-membership probabilities as the output of the model Softmax for multiclass classification added at the last (output) layer results in class-membership probabilities as the output of the model ReLU mainly used in the intermediate (hidden) layers of an NN to add non-linearities to the model References S. Raschka and V. Mirjalili. Python Machine Learning: Machine Learning and Deep Learning with Python, scikit-learn, and TensorFlow 2. Packt Publishing Ltd, Birmingham, third edition, 2019.","link":"/blog/2020/04/07/activation-functions-in-artificial-nns/"},{"title":"Atom Records in the Tripos Mol2 File","text":"A Tripos Mol2 file (.mol2) is a complete, portable representation of a SYBYL molecule. It is an ASCII file which contains all the information needed to reconstruct a SYBYL molecule. This post severs as a cheat sheet for the @&lt;TRIPOS&gt;ATOM record type indicator (RTI) and the SYBYL atom types in its data record. @&lt;TRIPOS&gt;ATOM Each data record associated with this RTI consists of a single data line. This data line contains all the information necessary to reconstruct one atom contained within the molecule. The atom ID numbers associated with the atoms in the molecule will be assigned sequentially when the .mol2 file is read into SYBYL. Format: 1atom_id atom_name x y z atom_type [subst_id [subst_name [charge [status_bit]]]] atom_id (integer) = the ID number of the atom at the time the file was created. This is provided for reference only and is not used when the .mol2 file is read into SYBYL. atom_name (string) = the name of the atom. x (real) = the x coordinate of the atom. y (real) = the y coordinate of the atom. z (real) = the z coordinate of the atom.•atom_type (string) = the SYBYL atom type for the atom. subst_id (integer) = the ID number of the substructure containing the atom. subst_name (string) = the name of the substructure containing the atom. charge (real) = the charge associated with the atom. status_bit (string) = the internal SYBYL status bits associated with the atom. These should never be set by the user. Valid status bits are DSPMOD, TYPECOL, CAP, BACKBONE, DICT, ESSENTIAL, WATER and DIRECT. Example 121 CA -0.149 0.299 0.000 C.3 1 ALA1 0.000 BACKBONE|DICT|DIRECT2 CA -0.149 0.299 0.000 C.3 In the first example the atom has ID number 1. It is named CA and is located at (-0.149, 0.299, 0.000). Its atom type is C.3. It belongs to the substructure with ID 1 which is named ALA1. The charge associated with the atom is 0.000 and the SYBYL status bits associated with the atom are BACKBONE, DICT, and DIRECT. Example two is the minimal information necessary for the MOL2 command to create an atom. SYBYL Atom Types The chemical atom types used in SYBYL are listed with their mnemonic code and a brief description. Code Definition Code Definition C.3 carbon sp3 H hydrogen C.2 carbon sp2 H.spc hydrogen in Single Point Charge (SPC) water model C.1 carbon sp H.t3p hydrogen in Transferable intermolecular Potential (TIP3P) water model C.ar carbon aromatic LP lone pair C.cat carbocation (C+) used only in a guadinium group Du dummy atom N.3 nitrogen sp3 Du.C dummy carbon N.2 nitrogen sp2 Any any atom N.1 nitrogen sp Hal halogen N.ar nitrogen aromatic Het heteroatom = N, O, S, P N.am nitrogen amide Hev heavy atom (non hydrogen) N.pl3 nitrogen trigonal planar Li lithium N.4 nitrogen sp3 positively charged Na sodium O.3 oxygen sp3 Mg magnesium O.2 oxygen sp2 Al aluminum O.co2 oxygen in carboxylate and phosphate groups Si silicon O.spc oxygen in Single Point Charge (SPC) water model K potassium O.t3p oxygen in Transferable Intermolecular Potential (TIP3P) water model Ca calcium S.3 sulfur sp3 Cr.th chromium (tetrahedral) S.2 sulfur sp2 Cr.oh chromium (octahedral) S.O sulfoxide sulfur Mn manganese S.O2 sulfone sulfur Fe iron P.3 phosphorous sp3 Co.oh cobalt (octahedral) F fluorine Cu copper Cl chlorine Zn zinc Br bromine Se selenium I iodine Mo molybdenum Sn tin References http://chemyang.ccnu.edu.cn/ccb/server/AIMMS/mol2.pdf","link":"/blog/2020/09/19/atom-records-in-the-tripos-mol2-file/"},{"title":"Attention Mechanism","text":"Here shows the essence of attention mechanism, independent of any frameworks. An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. Compactly, \\[ \\operatorname{Attention}(Q,K,V) = \\operatorname{Compatibility}(Q,K)V \\] ## Attention Categories The two most commonly used attention functions are additive attention, and dot-product (multiplicative) attention. Additive Attention \\[ \\operatorname{Attention}(Q,K,V) = \\operatorname{Network}(Q,K)V \\] Dot-Product Attention \\[ \\operatorname{Attention}(Q,K,V) = \\operatorname{softmax}(QK)V \\]","link":"/blog/2020/10/08/attention-mechanism/"},{"title":"Calculating Free Energy Differences Using Perturbation Theory","text":"Perturbation theory is one of the oldest and most useful, general techniques in applied mathematics. Its applications vary widely, and the free energy differences between to two similar systems can be calculated through this method. It is precisely the approach that was followed by the pioneers of free energy perturbation (FEP) theory. FEP is not only the oldest but also one of the more useful, general-purpose strategies for calculating free energy differences. In this post, we shall focus on calculating Helmholtz free energies in the canonical ensemble by virtue of FEP method. Extension to other ensembles is fairly straightforward. The Perturbation Formalism Let us start by considering an \\(N\\)-particle reference system described by the Hamiltonian \\(H_0 (\\mathbf{x}, \\mathbf{p})\\), which is a function of \\(3N\\) Cartesian coordinates, \\(\\mathbf{x}\\), and their conjugated momenta \\(\\mathbf{p}\\). We are interested in calculating the free energy difference between this system and the target system characterized by the Hamiltonian \\(H_1 (\\mathbf{x}, \\mathbf{p}) = H_0 (\\mathbf{x}, \\mathbf{p}) + \\Delta H (\\mathbf{x}, \\mathbf{p})\\). The difference in the Helmholtz free energy between the target and the reference systems, \\(\\Delta A\\), can be written in terms of the ratio of the corresponding partition functions, \\(Q_1\\) and \\(Q_0\\): \\[ \\begin{aligned} \\Delta A &amp;= -\\frac{1}{\\beta} \\ln \\frac{Q_{1}}{Q_{0}} = -\\frac{1}{\\beta} \\ln \\frac{\\iint \\mathrm{d} \\mathbf{x} \\mathrm{d} \\mathbf{p} \\exp \\left[-\\beta H_{1}(\\mathbf{x}, \\mathbf{p})\\right]}{\\iint \\mathrm{d} \\mathbf{x} \\mathrm{d} \\mathbf{p} \\exp \\left[-\\beta H_{0}(\\mathbf{x}, \\mathbf{p})\\right]} \\\\ &amp;= -\\frac{1}{\\beta} \\ln \\frac{\\iint \\mathrm{d} \\mathbf{x} \\mathrm{d} \\mathbf{p} \\exp \\left[-\\beta H_{0} (\\mathbf{x}, \\mathbf{p})\\right] \\exp \\left[-\\beta \\Delta H (\\mathbf{x}, \\mathbf{p})\\right]}{\\iint \\mathrm{d} \\mathbf{x} \\mathrm{d} \\mathbf{p} \\exp \\left[-\\beta H_{0}\\left(\\mathbf{x}, \\mathbf{p}\\right)\\right]}. \\end{aligned} \\tag{1} \\label{1} \\] For the canonical ensemble of the reference systems, the probability density function of finding the reference system in a state defined by positions \\(\\mathbf{x}\\) and momenta \\(\\mathbf{p}\\) is exactly \\[ P_{0} (\\mathbf{x}, \\mathbf{p}) = \\frac{\\exp \\left[-\\beta H_{0} (\\mathbf{x}, \\mathbf{p})\\right]}{\\iint \\mathrm{d} \\mathbf{x} \\mathrm{d} \\mathbf{p} \\exp \\left[-\\beta H_{0} (\\mathbf{x}, \\mathbf{p})\\right]}. \\tag{2} \\label{2} \\] With this definition, Eq. \\(\\eqref{1}\\) can be written compactly as \\[ \\Delta A = -\\frac{1}{\\beta} \\ln \\iint \\mathrm{d} \\mathbf{x} \\mathrm{d} \\mathbf{p} P_{0} (\\mathbf{x}, \\mathbf{p}) \\exp \\left[-\\beta \\Delta H (\\mathbf{x}, \\mathbf{p})\\right] = -\\frac{1}{\\beta} \\ln \\left\\langle\\exp \\left[-\\beta \\Delta H (\\mathbf{x}, \\mathbf{p})\\right]\\right\\rangle_{0}. \\tag{3} \\label{3} \\] If we reverse the reference and the target systems, and repeat the same derivation, using the same convention for \\(\\Delta H\\) as before, we obtain \\[ \\Delta A = \\frac{1}{\\beta} \\ln \\langle\\exp \\left[\\beta \\Delta H (\\mathbf{x}, \\mathbf{p})\\right]\\rangle_{1}. \\tag{4} \\label{4} \\] Interpretation of the Free Energy Perturbation Equation The formulas for free energy differences, Eq. \\(\\eqref{3}\\) and Eq. \\(\\eqref{4}\\), are formally exact for any perturbation. This does not mean, however, that they can always be successfully applied. To appreciate the practical limits of the perturbation formalism, we return to the expressions Eq. \\(\\eqref{3}\\) and Eq. \\(\\eqref{4}\\). Since \\(\\Delta A\\) is calculated as the average over a quantity that depends only on \\(\\Delta H\\), this average can be taken over the probability distribution \\(P_{0} (\\Delta H)\\) instead of \\(P_{0} (\\mathbf{x}, \\mathbf{p})\\). Then, \\(\\Delta A\\) in Eq. \\(\\eqref{3}\\) can be expressed as a one-dimensional integral over energy difference \\[ \\Delta A = -\\frac{1}{\\beta} \\ln \\int \\exp (-\\beta \\Delta H) P_{0}(\\Delta H) \\mathrm{d} \\Delta H. \\tag{5} \\label{5} \\] If \\(H_0\\) and \\(H_1\\) were the functions of a sufficient number of identically distributed random variables, then \\(\\Delta H\\) would be Gaussian distributed, which is a consequence of the central limit theorem. In practice, the probability distribution \\(P_{0} (\\Delta H)\\) deviates somewhat from the ideal Gaussian case, but still has a ‘Gaussian-like’ shape. Even though \\(P_{0} (\\Delta H)\\) is only rarely an exact Gaussian, it is instructive to consider this case in more detail. If we substitute \\[ P_{0}(\\Delta H) = \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp \\left[-\\frac{\\left(\\Delta H - \\langle\\Delta H\\rangle_{0}\\right)^{2}}{2 \\sigma^{2}}\\right], \\ \\text{where}\\ \\sigma^{2} = \\langle\\Delta H^2\\rangle_{0} - \\langle\\Delta H\\rangle_{0}^{2}. \\tag{6} \\label{6} \\] into Eq. \\(\\eqref{3}\\), we obtain \\[ \\exp (-\\beta \\Delta A) = \\frac{\\exp \\left[-\\beta\\left(\\langle\\Delta H\\rangle_{0}-\\frac{1}{2} \\beta \\sigma^{2}\\right)\\right]}{\\sqrt{2 \\pi} \\sigma} \\int \\exp \\left[-\\frac{\\left(\\Delta H-\\langle\\Delta H\\rangle_{0}-\\beta \\sigma^{2}\\right)^{2}}{2 \\sigma^{2}}\\right] \\mathrm{d} \\Delta H. \\tag{7} \\label{7} \\] Comparing Eq. \\(\\eqref{6}\\) and Eq. \\(\\eqref{7}\\), we note that \\(\\exp (-\\beta \\Delta H) P_{0}(\\Delta H)\\) is a Gaussian, as is \\(P_{0} (\\Delta H)\\), but is not normalized and shifted toward low \\(\\Delta H\\) by \\(\\beta \\sigma^2\\). This simple example clearly illustrates the limitations in the direct application of Eq. \\(\\eqref{5}\\). If the perturbation is significant and such that the variance \\(\\sigma\\) is large, the low-\\(\\Delta H\\) tail of the integrand, marked with stripes in the above figure is poorly sampled with \\(P_{0} (\\Delta H)\\) and, therefore, is known with low statistical accuracy. If \\(P_{0} (\\Delta H)\\) is Gaussian, there is, of course, no reason to carry out a numerical integration, since the integral in Eq. \\(\\eqref{7}\\) can be readily evaluated analytically. This yields \\[ \\Delta A = \\langle\\Delta H\\rangle_{0} - \\frac{1}{2} \\beta \\sigma^{2}. \\tag{8} \\label{8} \\] Gibbs–Bogoliubov Bounds on Free Energy Considering two spatial probability distribution functions, \\(F\\) and \\(G\\), on a space defined by \\(N\\) particles. We can show that \\[ \\int F \\ln F \\mathrm{d} \\mathbf{z}-\\int F \\ln G \\mathrm{d} \\mathbf{z} \\geq 0. \\tag{9} \\label{9} \\] Then substitute \\(P_{0} (\\mathbf{x}, \\mathbf{p})\\) and \\(P_{1} (\\mathbf{x}, \\mathbf{p})\\) as \\(F\\) and \\(G\\) into Eq. \\(\\eqref{9}\\), it obtains \\[ \\langle\\Delta H\\rangle_{1} \\leq \\Delta A \\leq \\langle\\Delta H\\rangle_{0} \\tag{10} \\label{10} \\] The Gibbs–Bogoliubov inequalities set bounds on \\(\\Delta A\\) of \\(\\langle\\Delta H\\rangle_{0}\\) and \\(\\langle\\Delta H\\rangle_{1}\\), which are easier a priori to estimate. These bounds are of considerable conceptual interest, but are rarely sufficiently tight to be helpful in practice. Cumulant Expansion of the Free Energy In this section, we will expand the desired free energy \\(\\Delta A\\) in a series with respect to the small parameter \\(\\beta\\). The basic idea of the derivation is the cumulant expansion in probability theory, which leads \\[ \\Delta A = -\\frac{1}{\\beta} \\ln \\left\\langle\\exp \\left[-\\beta \\Delta H (\\mathbf{x}, \\mathbf{p})\\right]\\right\\rangle_{0} = \\sum_{n=1}^{\\infty} \\frac{(-\\beta)^{n-1}}{n!} \\langle\\Delta H^{n}\\rangle_{\\text{c}}. \\tag{11} \\label{11} \\] The first four cumulants are called the mean, variance, skewness, and curtosis (or kurtosis) of the distribution, respectively, and are obtained from the moments as \\[ \\begin{array}{l}{\\langle\\Delta H\\rangle_{\\text{c}} = \\langle\\Delta H\\rangle_{0}} \\\\ {\\langle\\Delta H^{2}\\rangle_{\\text{c}} = \\left\\langle\\Delta H^{2}\\right\\rangle_{0}-\\langle\\Delta H\\rangle_{0}^{2}} \\\\ {\\langle\\Delta H^{3}\\rangle_{\\text{c}} = \\left\\langle\\Delta H^{3}\\right\\rangle_{0}-3\\left\\langle\\Delta H^{2}\\right\\rangle_{0}\\langle\\Delta H\\rangle_{0} + 2\\langle\\Delta H\\rangle_{0}^{3}} \\\\ {\\langle\\Delta H^{4}\\rangle_{\\text{c}} = \\left\\langle\\Delta H^{4}\\right\\rangle_{0}-4\\left\\langle\\Delta H^{3}\\right\\rangle_{0}\\langle\\Delta H\\rangle_{0}-3\\langle\\Delta H\\rangle_{0}^{2} + 12\\left\\langle\\Delta H^{2}\\right\\rangle_{0}\\langle\\Delta H\\rangle_{0}^{2}-6\\langle\\Delta H\\rangle_{0}^{4}}\\end{array} \\tag{12} \\label{12} \\] As may be seen, the formulas for higher-order cumulants become more complicated. More importantly, they are increasingly difficult to estimate accurately from simulations. If the expansion is terminated after the second order, the free energy takes the form \\[ \\Delta A = \\langle\\Delta H\\rangle_{0}-\\frac{\\beta}{2}\\left(\\left\\langle\\Delta H^{2}\\right\\rangle_{0}-\\langle\\Delta H\\rangle_{0}^{2}\\right), \\tag{13} \\label{13} \\] which is identical to Eq. \\(\\eqref{8}\\). This means that the second-order perturbation theory is accurate for Gaussian probability distribution functions. In other words, truncating the expansion for \\(\\Delta A\\) at the second order is equivalent to replacing \\(P_{0} (\\Delta H)\\) by a Gaussian distribution with the same variance. This is a fundamental result because it forms the basis for many approximate methods for estimating free energies. Dealing with Large Perturbations Note that using Eq. \\(\\eqref{3}\\) directly can be successful only if \\(P_0 (\\Delta H)\\) is a narrow function of \\(\\Delta H\\). This difficulty in applying FEP theory can be circumvented through a simple stratification strategy, also often called staging. It relies on constructing several intermediate states between the reference and the target state such that \\(P (\\Delta H_{i})\\) for two consecutive states \\(i\\) and \\(i + 1\\) sampled at state \\(i\\) is sufficiently narrow for the direct evaluation of the corresponding free energy difference, \\(\\Delta A_{i}\\). Then, Eq. \\(\\eqref{3}\\) can be used serially to yield \\(\\Delta A\\). If we construct \\(N - 1\\) intermediate states then \\[ \\Delta A = \\sum_{i=0}^{N-1} \\Delta A_{i} = -\\frac{1}{\\beta} \\sum_{i=0}^{N-1} \\ln \\left\\langle\\exp \\left(-\\beta \\Delta H_{i}\\right)\\right\\rangle_{i}. \\tag{14} \\label{14} \\] Intermediate states do not have to be physically meaningful, i.e., they do not have to correspond to systems that actually exist. More generally, we can consider the Hamiltonian as a function of some parameter, \\(\\lambda\\). Without loss of generality, we can choose \\(0 \\le \\lambda \\le 1\\), such that \\(\\lambda = 0\\) and \\(\\lambda = 1\\) for the reference and target states, respectively. A simple choice for the dependence of the Hamiltonian on \\(\\lambda\\) is a linear function \\[ H (\\lambda_{i}) = \\lambda_{i} H_{1} + (1-\\lambda_{i}) H_{0} = H_{0} + \\lambda_{i} \\Delta H, \\tag{15} \\label{15} \\] which justifies calling λ a coupling parameter. If we create \\(N - 1\\) intermediate states linking the reference and the target states such that \\(\\lambda_0 = 0\\) and \\(\\lambda_N = 1\\) then the change in the Hamiltonian, \\(\\Delta H_i\\), between two consecutive states is given by \\[ \\Delta H_{i} = H (\\lambda_{i+1}) - H (\\lambda_{i}) = (\\lambda_{i+1} - \\lambda_{i}) \\Delta H = \\Delta \\lambda_{i} \\Delta H, \\tag{16} \\label{16} \\] where \\(\\Delta \\lambda_{i} = \\lambda_{i+1} − \\lambda_{i}\\), and the formula for the total free energy difference becomes \\[ \\Delta A = -\\frac{1}{\\beta} \\sum_{i=0}^{N-1} \\ln \\left\\langle\\exp \\left(-\\beta \\Delta \\lambda_{i} \\Delta H\\right)\\right\\rangle_{\\lambda_{i}}. \\tag{17} \\label{17} \\] Stratification is not specific to FEP – it is a universal strategy that improves the efficiency of many other methods for calculating free energies. Alchemical Transformations Free Energies of Binding The free energy of binding of two molecules, \\(\\Delta A_{\\text{binding}}\\), defined as the free energy difference between these molecules in the bound and the free, unbound states, can be determined experimentally through the measurement of binding constants using, for instance, BIAcore or microcalorimetry techniques. Computationally, it can be evaluated through directly transformation or thermodynamic cycles. Absolute Binding Free Energy The thermodynamic cycle for the determination of protein-ligand absolute binding free energy. In general, FEP cannot be used for calculating \\(\\Delta A_{\\text{binding}}\\) directly, following the upper horizontal transformation. Considering that the lower horizontal transformation corresponds to a zero free energy change, annihilation of the ligand in the reference, free state – i.e., the left, vertical transformation, and in the bound state – i.e., the right, vertical transformation, yields the binding free energy: \\(\\Delta A_{\\text{binding}} = \\Delta A_{\\text{annihilation,0}} - \\Delta A_{\\text{annihilation,1}}\\). The contribution \\(\\Delta A_{\\text{restrain}}\\) that appears in the reverse, lower horizontal transformation characterizes the loss of rotational and translational entropies due to restraining the position of the ligand. Relative Binding Free Energy The thermodynamic cycle for the determination of protein-ligand relative binding free energy. Instead of carrying the horizontal transformations, one can mutate the ligand in the free state – i.e., the left, vertical ‘alchemical transformation’, and in the bound state – i.e., the right, vertical ‘alchemical transformation.’ This yields the difference in the binding free energies: \\(\\Delta A_{\\text{binding,B}} - \\Delta A_{\\text{binding,A}} = \\Delta A_{\\text{mutation,1}} - \\Delta A_{\\text{mutation,0}}\\). References Ch. Chipot and A. Pohorille. Free Energy Calculations: Theory and Applications in Chemistry and Biology. Springer, Berlin, 2007.","link":"/blog/2020/01/30/calculating-free-energy-differences-using-perturbation-theory/"},{"title":"Computer-Aided Synthesis Planning: A Brief Introduction","text":"Computer-aided synthesis planning (CASP) is focused on the goal of accelerating the process by which chemists decide how to synthesize small molecule compounds. The ideal CASP program would take a molecular structure as input and output a sorted list of detailed reaction schemes that each connect that target to purchasable starting materials via a series of chemically feasible reaction steps.1 This post is a concise note of the article \"Automatic retrosynthetic route planning using template-free models\"2. Computer-aided retrosynthetic route planning strategies can be clustered into two main categories: template-based and template-free methods. Template-based methods can also be categorized as using either a manual encoding approach or an automated extraction approach. CASP History The course of CASP development: 1969, E. J. Corey, Logic and Heuristics Applied to Synthetic Analysis (LHASA)3,4, proprietary software. 2017, Marwin H. S. Segler, Neural‐Symbolic Machine Learning5, code reproduced by Coley. 2017, Connor W. Coley, similarity-based model6, code link. 2017, Bowen Liu, LSTM-based seq2seq model7, code link. 2017, Ashish Vaswani, multi-head attention-based Transformer model (state-of-the-art translation model)8. 2018, Marwin H. S. Segler, DNN and symbolic AI9. 2019, Baylon and coworkers, deep learning using multiscale reaction classification, template-based model10, code and data are unavailable. 2020, Kangjie Lin, Transformer-based seq2seq model2, code link. Retrosynthesis Software A complete retrosynthetic program should be made up of five components (mentioned by Coley et al.): a library containing the disconnection rules a recursive application engine that generates candidate reactants for target compounds a building block database containing available compounds to act as terminal nodes a strategy to guide the retrosynthetic search a scoring function for the single-step or pathway Several available retrosynthesis software are listed in Wikipedia11. Please refer to it for more information, and here only shows those of supreme importance for me. LHASA LHASA (Logic and Heuristics Applied to Synthetic Analysis) is a program for synthesis planning, an expert system to assist chemists in designing efficient routes to target molecules for organic synthesis. Operating differently from reaction retrieval systems, LHASA searches its own way in synthesizing known and unknown compounds, using a chemical knowledge base (rather than a database of literature examples). Since LHASA operates in a rigorously retrosynthetic fashion, the knowledge base contains information about retro-reactions (or transforms) rather than reactions. The current version of LHASA (20.3) contains 2271 transforms and 495 so-called tactical combinations. TAGS: Template-Based Model Chematica Chematica is a software/database that uses algorithms and a collective database of 250 years of organic chemical information to predict and provide synthesis pathways for molecules. The software development, led by Bartosz A. Grzybowski, took place for a decade when it was finally publicized in August, 2012. In 2017, the software and database were wholly purchased by Merck KGaA | MRK. The software has been made available commercially since the acquisition as Synthia.12 TAGS: Template-Based Model Synthia Synthia (formerly Chematica), one of the most well-known, expert-encoded, template-based retrosynthetic analysis tools, is a commercial program developed by Grzybowski and coworkers. This tool uses a manually collected knowledge database containing about 70 000 hand-encoded reaction transformation rules. TAGS: Template-Based Model AutoSynRoute AutoSynRoute is an automatic data-driven retrosynthetic route planning system developed by Kangjie Lin and Youjun Xu, which includes retrosynthesis task prediction using a Transformer-based seq2seq model and MCTS with heuristic scoring for route planning. Unlike other template-based methods, which either rely on experts' laborious work or simple, contextless rule-based systems, this approach is fully end-to-end and naturally incorporates the global molecular context of the reaction species. PERFORMANCE: Prediction Accuracy: 63.0%, Validity Rate of SMILES: 99.6%. TAGS: Template-Free, Transformer, seq2seq. Reaction Database Reaxys Related Literature Reviews 2016, Computer-Assisted Synthetic Planning: The End of the Beginning13 2018, Machine Learning in Computer-Aided Synthesis Planning1 References Connor W. Coley, William H. Green, and Klavs F. Jensen. Machine learning in computer-aided synthesis planning. Accounts of Chemical Research, 51(5):1281–1289, may 2018. Kangjie Lin, Youjun Xu, Jianfeng Pei, and Luhua Lai. Automatic retrosynthetic route planning using template-free models. Chemical Science, 11(12):3355–3364, 2020. E. J. Corey and W. T. Wipke. Computer-assisted design of complex organic syntheses. Science, 166(3902):178–192, oct 1969. E. Corey, A. Long, and S. Rubenstein. Computer-assisted analysis in organic synthesis. Science, 228(4698):408–418, apr 1985. Marwin H. S. Segler and Mark P. Waller. Neural-symbolic machine learning for retrosynthesis and reaction prediction. Chemistry - A European Journal, 23(25):5966–5971, feb 2017. Connor W. Coley, Luke Rogers, William H. Green, and Klavs F. Jensen. Computer-assisted retrosynthesis based on molecular similarity. ACS Central Science, 3(12):1237–1245, nov 2017. Bowen Liu, Bharath Ramsundar, Prasad Kawthekar, Jade Shi, Joseph Gomes, Quang Luu Nguyen, Stephen Ho, Jack Sloane, Paul Wender, and Vijay Pande. Retrosynthetic reaction prediction using neural sequence-to-sequence models. ACS Central Science, 3(10):1103–1113, sep 2017. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. arxiv:1706.03762, 2017. Marwin H. S. Segler, Mike Preuss, and Mark P. Waller. Planning chemical syntheses with deep neural networks and symbolic AI. Nature, 555(7698):604–610, mar 2018. Javier L. Baylon, Nicholas A. Cilfone, Jeffrey R. Gulcher, and Thomas W. Chittenden. Enhancing retrosynthetic reaction prediction with deep learning using multiscale reaction classification. Journal of Chemical Information and Modeling, 59(2):673–688, jan 2019. WIKIPEDIA: List of computer-assisted organic synthesis software WIKIPEDIA: Chematica Sara Szymkuć, Ewa P. Gajewska, Tomasz Klucznik, Karol Molga, Piotr Dittwald, Michał Startek, Michał Bajczyk, and Bartosz A. Grzybowski. Computer-assisted synthetic planning: The end of the beginning. Ange- wandte Chemie International Edition, 55(20):5904–5937, apr 2016.","link":"/blog/2020/10/09/computer-aided-synthesis-planning-a-brief-introduction/"},{"title":"Conda Local-Package-Auxiliary Installation Scheme","text":"Installing Conda packages is really a headache problem, due to the strikingly slow Internet speed. Through tireless endeavor, I finally find a nice offline approach to get around it. CLPAI Scheme Procedures Execute conda install xxx to install the desired package. When it raises \"CondaHTTPError\", please copy the links into Manager to download those failed entries. Move the downloaded files to ~/usr/local/anaconda3/pkgs directory, and make sure that they possess consistent filenames. Next add the failed links to ~/usr/local/anaconda3/pkgs/urls.txt file. Finally rerun the conda install xxx command, and afterwards the package will be successfully installed.","link":"/blog/2020/04/12/conda-local-package-auxiliary-installation-scheme/"},{"title":"Free Energy Simulation Workflow","text":"Numerical simulations for the determination of free energy difference in protein–ligand binding is of vital importance in the field of computer-aided drug design. This article illustrates the workflow of free energy simulation, aiming at offering an idea for the designment of related software. Generally speaking, the workflow of free energy simulation can be separated as four procedures, namely a series of preparations, simulation setup, production MD and result analysis, which will be presented in great detail in the following sections. 1 Structure Preparation In general, the typical structure file from the PDB is not suitable for immediate use in molecular modeling calculations. And hence one must be appropriately prepared before implementing certain simulations. 1.1 Protein Preparation A typical PDB structure file consists only of heavy atoms and may include a cocrystallized ligand, water molecules, metal ions, and cofactors. Some structures are multimeric, and may need to be reduced to a single unit. Usually, there is a list of possible defects in a raw PDB file: miss hydrogen atoms have incorrect bond order assignments, charge states, or orientations of various groups miss information on connectivity, bond orders and formal charges In view of this, we must develop a tool (suggested name: ProPrep) to manipulate protein structure files. Some open source software, such as Harms Lab PDBtools, HADDOCK pdb-tools and rDock, can be referred to when creating a new PDB file manipulator. The desired functions in this procedure is presented as follows. First, a PDB file should imported, and the molecular graphics panel displays all ligand hydrogens, but only the polar hydrogens on the receptor by default. Then executing automated processing, manual modification, or both as the circumstances may require. Finally, saving the well processed structure for future use. 1.2 Ligand Preparation Similarly, ligand files can be sourced from numerous places, such as vendors or databases, often in the form of 1D or 2D structures with unstandardized chemistry. Therefore, another tool (suggested name: LigPrep) that can convert ligand files to 3D structures, with the chemistry properly standardized and extrapolated, ready for use in virtual screening should be developed as well. Also, it is better for this program to be capable of producing multiple output structures for each input structure by generating different protonation states, stereochemistry, tautomers, and ring conformations. 2 Simulation Setup 2.1 Basic Jobs in the Setup Step When structures are properly prepared, we can generate input files for MD running. The basic jobs should be done here is as follows. Parameterizing force field. Creating topology and coordinate files. Adjusting the density and provide starting velocities via running minimization and MD protocols. Generating ligand morph pairs according to perturbation maps. Combining the ligand morph pairs with a receptor to form complex morph pairs. Setting coupling parameters \\(\\lambda\\) for the multi-step perturbation approach. 2.2 FESetup: An Automatic Setup Program FESetup is one of the programs designed for automating the setup of relative alchemical free energy (AFE) simulations such as free energy perturbation (FEP) and thermodynamic integration (TI). The following shows its fundamental workflow. FESetup Workflow The ligand is automatically parametrized with antechamber (the default force field is GAFF). And atomic partial charges are by default derived with the AM1−BCC method, which is achieved through sqm, a semi-empirical tool in the AmberTools. Protonation, and more specifically the tautomeric, state of the ligand is the responsibility of the user. The receptor can optionally be protonated with PROPKA3 After parametrization, LEaP is recruited to create topology and coordinate files for either vacuum or a solvated simulation box with counterions. Preset minimization and MD protocols can be carried out to adjust the density and provide starting velocities via an abstraction interface that supports a number of MD engines: AMBER, GROMACS, NAMD, or DL_POLY. Two ligands can be combined into a morph pair. FESetup will determine the maximum common substructure and use this to set up the mapped region with a set of common coordinates. Any atoms not mapped in this way are described as softcore (dummy) atoms. The code will create all necessary topology and coordinate files for the perturbed simulation for the MD packages AMBER, GROMACS, and Sire. NAMD is principally supported too because NAMD can read AMBER files butFESetup Workflow it is dual-topology only. The receptor can be combined with a morph to form a complex-morph. The solvated box is created from the coordinates of the unperturbed simulation system. The only additional atoms and their coordinates are those for the dummy atoms. These will be computed from the internal coordinates of the other state with the existing atoms. Note: Toolkits used by FESetup Various third-party software and toolkits are recruited for setup: Openbabel is used to convert file formats, carry out preliminary minimization, and possibly create alternative conformations. RDKit is used to compute the MCS between a ligand pair (a “morph”) with the fmcs algorithm. Sire is used to read AMBER topology files and provide data structures for force field parameters and allow for their manipulation. Furthermore, Sire can detect and select internal degrees of freedom for Monte Carlo (MC) simulations. The AmberTools antechamber and LEaP are used to create AMBER topology files including force field parametrization. 3 Production MD 3.1 Minimization and Equilibrium MD This step is carried out for ensuring that the system has no steric clashes or inappropriate geometry and possess the proper temperature and density. 3.2 Biased Sampling Biased Sampling is implemented to overcome the problem of quasi-ergodicity 3.3 Job Submission It always takes long time to execute production MD, hence suitably submitting the jobs on a cluster or local host is of vital importance. This function is just designed for the purpose. 4 Result Analysis 4.1 Free Energy Estimation Estimating the free energy, 4.2 Error and Correlation Evaluating the calculated results. References Documentation for The Protein Preparation Wizard Documentation for Schrödinger LigPrep Panel Alchemical Analysis: An open tool implementing some recommended practices for analyzing alchemical free energy calculations","link":"/blog/2020/09/14/free-energy-simulation-workflow/"},{"title":"Generating BibTeX from DOI, arXiv or ISBN","text":"In order to automate the formation of specifically formatted references, some kinds of BibTeX converters is indispensable. DOI to BibTeX Converter Bibcure Bibcure helps in boring tasks by keeping your bibfile up to date and normalized...also allows you to easily download all papers inside your bibtex. Install it using pip: installation1$ pip install bibcure This package includes a doi2bib tool, which can be taken to convert DOI to BibTeX. doi2bib Given a DOI number... 1$ doi2bib 10.1038/s41524-017-0032-0 get bib item given a DOI (requires internet connection) You can easily append a bib into a bibfile, just do: 1$ doi2bib 10.1038/s41524-017-0032-0 &gt;&gt; file.bib You also can generate a bibtex from a txt file containing a list of DOIs: 1$ doi2bib --input file_with_dois.txt --output refs.bib arXiv to BibTeX Converter Bibcure Bibcure also contains a arxivcheck tool, which can be used to convert arXiv to BibTeX. arxivcheck Given an arXiv id... 1$ arxivcheck 1601.02785 check if has been published, and then returns the updated bib (requires internet connection). Given a title... 1$ arxivcheck --title &quot;A useful paper with hopefully unique title published on arxiv&quot; search papers related and return a bibtex file for the first item. You can easily append a bib into a bibfile, just do 1$ arxivcheck --title &quot;A useful paper with hopefully unique title published on arxiv&quot; &gt;&gt; file.bib You also can interact with results, just pass --ask parameter: 1$ arxivcheck --ask --title &quot;A useful paper with hopefully unique title published on arxiv&quot; ISBN to BibTeX Converter Search for ISBN and export to BibTeX Ottobib: http://www.ottobib.com/ similar to Ottobib: http://manas.tungare.name/software/isbn-to-bibtex/ Search in all fields and export to BibTeX needs many mouse clicks until you can export: http://www.citeulike.org/ Famous but needs repair lead.to/amazon select Amazon.co.jp and Bibtex. This combination still works today. All databases have many mistakes in the fields and you will have to double check always. But don't worry the major science journals provide bad BibTeX data too. References What is the best way to get BibTeX entries from ISBN number?","link":"/blog/2020/05/14/generating-bibtex-from-doi-arxiv-or-isbn/"},{"title":"How to Enable SSH on Ubuntu 18.04","text":"Secure Shell (SSH) is a cryptographic network protocol used for a secure connection between a client and a server. In this tutorial, we'll show you how to enable SSH on an Ubuntu Desktop machine. Enabling SSH will allow you to remotely connect to your Ubuntu machine and securely transfer files or perform administrative tasks. Prerequisites The SSH server is not installed by default on Ubuntu desktop systems but it can be easily installed from the standard Ubuntu repositories. To install SSH on your Ubuntu system completes the following command: 1$ sudo apt install openssh-server Enabling SSH on Ubuntu Once the installation is completed, the SSH service will start automatically. To verify that the installation was successful and SSH service is running type the following command which will print the SSH server status: 1$ sudo systemctl status ssh You should see something like Active: active (running). SSH Status Ubuntu comes with a firewall configuration tool called UFW. If the firewall is enabled on your system, make sure to open the SSH port: 1$ sudo ufw allow ssh Connecting to SSH Over LAN To connect to your Ubuntu machine over LAN you only need to enter the following command: Change the username with the actual user name and ip_address with the IP Address of the Ubuntu machine where you installed SSH. If you don't know your IP address you can easily find it using the ip command: 1$ ip a ip a As you can see from the output, the system IP address is 192.168.0.105. Once you’ve found the IP address, login to remote machine by running the following ssh command: 1$ ssh leoy@192.168.0.105 Connecting to SSH Over Internet To connect to your Ubuntu machine over the Internet you will need to know your public IP Address and to configure your router to accept data on port 22 and send it to the Ubuntu machine where the SSH is running. To determine the public IP address of the machine you’re trying to SSH to, simply visit the following URL: https://ifconfig.co/ip. Once you’ve found the IP address, and configured your router you can log in by typing: 1$ ssh username@public_ip_address You can also set up an SSH key-based authentication and connect to your Ubuntu machine without entering a password. Disabling SSH on Ubuntu If for some reason you want to disable SSH on your Ubuntu machine you can simply stop the SSH service by running: 1$ sudo systemctl stop ssh To start it again run: 1$ sudo systemctl start ssh To disable the SSH service to start during system boot run: 1$ sudo systemctl disable ssh To enable it again type: 1$ sudo systemctl enable ssh Set up Passwordless SSH Login on Ubuntu There’re basically two ways of authenticating user login with OpenSSH server: password authentication and public key-based authentication. The latter is also known as passwordless SSH login because you don’t have to enter your password. Step 1: generate authentication key 1$ ssh-keygen Step 2: upload the public key to remote Linux server 1$ ssh-copy-id remote-user@server-ip References https://linuxize.com/post/how-to-enable-ssh-on-ubuntu-18-04/ https://linuxize.com/post/how-to-enable-ssh-on-ubuntu-20-04/ https://www.linuxbabe.com/linux-server/setup-passwordless-ssh-login","link":"/blog/2020/01/26/how-to-enable-ssh-on-ubuntu-18-04/"},{"title":"How to Install R on Ubuntu 18.04","text":"R is a fast growing open-source programming language and free environment that specializes in statistical computing and graphical representation. It is supported by the R Foundation for Statistical Computing and mainly used by statisticians and data miners for developing statistical software and performing data analysis. This tutorial will guide you through the steps of installing R on an Ubuntu 18.04 machine. Prerequisites Before you get started with this tutorial, you’ll need an Ubuntu 18.04 machine with: at least 1G of RAM. If your system has less than 1GB of RAM, you can create a swap file . a non-root user with sudo privileges . Installing R on Ubuntu At the time of writing this article, the latest stable version of R is version 3.5. The R packages from the Ubuntu repositories are often outdated so we’ll install R by adding the repository maintained by CRAN . To install the latest stable version of R on Ubuntu 18.04, follow these steps: Install the packages necessary to add a new repository over HTTPS: 1$ sudo apt install apt-transport-https software-properties-common Enable the CRAN repository and add the CRAN GPG key to your system using the following commands: 12$ sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys E298A3A825C0D65DFD57CBB651716619E084DAB9$ sudo add-apt-repository 'deb https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/' Now that the apt repository is added , update the packages list and install the R package by typing: 12$ sudo apt update$ sudo apt install r-base To verify that the installation was successful run the following command which will print the R version: 1$ R --version Out:123456789R version 3.5.1 (2018-07-02) -- &quot;Feather Spray&quot;Copyright (C) 2018 The R Foundation for Statistical ComputingPlatform: x86_64-pc-linux-gnu (64-bit)R is free software and comes with ABSOLUTELY NO WARRANTY.You are welcome to redistribute it under the terms of theGNU General Public License versions 2 or 3.For more information about these matters seehttp://www.gnu.org/licenses/. Installing R Packages from CRAN One of the main reasons why R is so popular is the wide array of packages available through the Comprehensive R Archive Network (CRAN). Install the build-essential package which contains the tools required for compiling R Packages. 1$ sudo apt install build-essential For demonstration purposes, we’ll install a package named stringr , which provides fast, correct implementations of common string manipulations. When started as root the packages will be installed globally and available for all system users. If you start R without sudo, a personal library will be set up for your user. Start by opening the R console as root: 1234567891011121314151617181920$ sudo -i RR version 3.5.1 (2018-07-02) -- &quot;Feather Spray&quot;Copyright (C) 2018 The R Foundation for Statistical ComputingPlatform: x86_64-pc-linux-gnu (64-bit)R is free software and comes with ABSOLUTELY NO WARRANTY.You are welcome to redistribute it under certain conditions.Type 'license()' or 'licence()' for distribution details. Natural language support but running in an English localeR is a collaborative project with many contributors.Type 'contributors()' for more information and'citation()' on how to cite R or R packages in publications.Type 'demo()' for some demos, 'help()' for on-line help, or'help.start()' for an HTML browser interface to help.Type 'q()' to quit R.&gt; All the following commands are executed within the R console. Install the stringr package by typing: 1install.packages(&quot;stringr&quot;) The installation will take some time and once complete, load the library with: 1library(stringr) Create a simple character vector named tutorial: 1tutorial &lt;- c(&quot;How&quot;, &quot;to&quot;, &quot;Install&quot;, &quot;R&quot;, &quot;on&quot;, &quot;Ubuntu&quot;, &quot;18.04&quot;) Run the following function which prints the length of a string: 1str_length(tutorial) Out:1[1] 3 2 7 1 2 6 5 You can find more R packages at Available CRAN Packages By Name and install them with install.packages(). Using R language with Anaconda Creating an environment with R Download and install Anaconda. Create a new conda environment with all the r-essentials conda packages built from CRAN: conda create -n r_env r-essentials r-base Activate the environment: conda activate r_env List the packages in the environment: conda list The list shows that the package r-base is installed and r is listed in the build string of the other R packages in the environment. Anaconda Navigator, the Anaconda graphical package manager and application launcher, creates R environments by default. Conclusion You have successfully installed R your Ubuntu system and learned how to install R packages. If you hit a problem or have feedback, leave a comment below. References https://linuxize.com/post/how-to-install-r-on-ubuntu-18-04/ https://docs.anaconda.com/anaconda/user-guide/tasks/using-r-language/","link":"/blog/2020/09/19/how-to-install-r-on-ubuntu-18-04/"},{"title":"图神经网络简介","text":"近年来，图神经网络（GNNs）在社交网络、知识图谱、推荐系统甚至生命科学等各个领域得到越来越广泛的应用。本文将对图神经网络的一般框架，即消息传递形式体系，进行介绍；并在此基础上讨论一些基础的图神经网络模型。 前言 深度学习在许多领域取得了可喜的进展，如计算机视觉和自然语言处理。这些任务中的数据通常在欧几里得域中表示。然而，许多学习任务需要处理包含丰富元素间关系信息的非欧几里得的图数据，如建模物理系统、学习分子指纹、预测蛋白质界面等[1]。图神经网络（GNNs）是图域上的深度学习模型，由于其令人信服的性能和较高的可解释性，已成为近年来广泛应用的图分析方法[2]。这篇贴子提供了对图神经网络的基本概念、模型和应用的简单介绍。 消息传递形式体系 图神经网络是一种定义在图数据\\(\\mathbb{G} = (\\mathbb{V}; \\mathbb{E})\\)上深度神经网络的一般框架，通过自动学习节点嵌入、图嵌入作为图的表示，进而做出预测。其关键思想是，通过机器学习生成仅依赖于图形结构以及可能拥有的任何特征信息的节点表示。各种图神经网络可以表示为一种邻域聚集（neighborhood aggregation）或消息传递（message passing）。在这个框架下，图神经网络通过两个步骤将图形映射到输出。首先，有一个传播步骤，计算每个节点的节点表示；其次，输出模型\\(\\hat{y}_{v} = g(h_v; x_v)\\)从节点表示和相应的标签映射到每个节点的输出\\(o_v\\)。 消息传递的前向传播有两个阶段，消息传递阶段（message passing phase）和读出阶段（readout phase）[3]。消息传递阶段通常经过\\(T\\)时间步的迭代，单个时间步根据消息函数\\(\\mathcal{M}_{t}\\)和顶点更新函数\\(\\mathcal{U}_{t}\\)定义。在消息传递阶段，图中每个节点的隐藏状态\\(h_{t+1,v_{i}}\\)根据消息\\(m_{t+1,v_{i}}\\)更新[3]： \\[ m_{t+1,v_{i}} = \\mathcal{A}(\\{\\mathcal{M}_{t}(h_{t,v_{i}}, h_{t,v_{j}}, h_{t,e_{ij}}) | v_{j} \\in \\mathcal{N}(v_{i})\\}), \\] \\[ h_{t+1,v_{i}} = \\mathcal{U}_{t}(h_{t,v_{i}}, m_{t+1,v_{i}}). \\] 其中\\(\\mathcal{A}\\)为聚集方案，例如sum、mean、max；\\(\\mathcal{N}(v_{i})\\)表示图\\(\\mathbb{G}\\)中\\(v_{i}\\)的邻居。当有多个边类型时，我们必须定义多个消息函数\\(\\mathcal{M}_{t,\\operatorname{type}(e_{ij})}\\)，它是\\(t\\)层边类型为\\(\\operatorname{type}(e_{ij})\\)的消息函数。 读出阶段使用一种读出函数\\(\\mathcal{O}\\)计算整个图的特征矢量（feature vector），也就是嵌入（embedding）。 \\[ \\hat{y} = \\mathcal{O}(\\{h_{v} ; x_{v} | v \\in \\mathbb{V}\\}). \\] 这种香草形式体系是由Scarselli等人[2009]首先提出的[4]，后来被称为消息传递范式[3]。 图卷积神经网络 卷积神经网络（CNNs）可以提取出数据的空间关联信息，因此在深度学习领域取得了巨大的成功。自然地，我们也可以尝试将其推广至图，学习节点间的空间关联，从而提取图的结构信息。在这个方向上的研究通常可以归类为谱方法（spectral approaches）和空间方法（spatial approaches）[1]。 Bruna等人[2014]提出了谱网络[5]。通过计算图的拉普拉斯特征分解，在傅里叶域中定义了卷积运算。操作可以被定义为一个信号\\(\\mathbf{x} \\in \\mathbb{R}^{N}\\)与一个由\\(\\mathbf{\\theta} \\in \\mathbb{R}^{N}\\)参数化的过滤器\\(\\mathbf{g}_{\\theta} = \\operatorname{diag}(\\mathbf{\\theta})\\)的乘积： \\[ \\mathbf{g}_{\\theta} * \\mathbf{x} = \\mathbf{U} \\mathbf{g}_{\\theta}(\\Lambda) \\mathbf{U}^{\\text{T}} \\mathbf{x}, \\] 式中，\\(\\mathbf{U}\\)是对称正规化的图拉普拉斯\\(\\mathbf{L} = \\mathbf{I}_{N} - \\mathbf{D}^{-\\frac{1}{2}} \\mathbf{A} \\mathbf{D}^{-\\frac{1}{2}} = \\mathbf{U} \\Lambda \\mathbf{U}^{\\text{T}}\\)的本征矢量矩阵，\\(\\Lambda\\)是对角线为本征值的对角矩阵。 不过这个操作的计算难度特别高，所以必须做近似处理，降低计算消耗。Hammond等人[2011]指出\\(\\mathbf{g}_{\\theta}(\\Lambda)\\)可以用一个截断的切比雪夫多项式\\(\\mathbf{T}_{k}(x)\\)近似[6]。根据此方法，Defferrard等人[2016]提出了ChebNet，它使用\\(K\\)-局域卷积来定义卷积神经网络，可以消除对拉普拉斯特征向量的计算[7]。 后来Kipf和Welling[2017]将卷积操作只展开到\\(k = 1\\)阶，并做了一些其他近似，然后提出了至今广泛使用的图卷积神经网络（GCN）[8]。利用消息传递的框架，我们可以将其表述成： \\[ m_{t+1,v_{i}} = \\sum_{v_{j} \\in \\mathcal{N}(v_{i}) \\cup v_{i}} \\frac{W_{t} h_{t,v_{j}}}{\\sqrt{\\operatorname{deg}(v_{i}) \\operatorname{deg}(v_{j})}}, \\] \\[ h_{t+1,v_{i}} = \\operatorname{ReLU}(W_{t}^{\\prime} m_{t+1,v_{i}}). \\] 也就是说，在邻近的节点特征或者隐藏状态是先由权重矩阵\\(W_{t}\\)做线性变化，再通过度的规范化，最后加起来（包含自身节点）得到局部图结构下的消息。这个消息然后传到一个全连接的神经网络层生成新的节点状态（隐藏状态或者嵌入）。为了更好地理解图卷积神经网络，我们可以参考PyTorch geometric的GCN实现PyTorch geometric: GCNConv： 1234567891011121314151617181920import torchfrom torch_geometric.nn import MessagePassingfrom torch_geometric.utils import add_self_loops, degreeclass GCNConv(MessagePassing): def __init__(self, in_channels, out_channels): super(GCNConv, self).__init__(aggr='add') self.lin = torch.nn.Linear(in_channels, out_channels) def forward(self, x, edge_index): edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0)) x = self.lin(x) row, col = edge_index deg = degree(col, x.size(0), dtype=x.dtype) deg_inv_sqrt = deg.pow(-0.5) norm = deg_inv_sqrt[row] * deg_inv_sqrt[col] return self.propagate(edge_index, x=x, norm=norm) def message(self, x_j, norm): return norm.view(-1, 1) * x_j 最后值得注意的是，作为谱方法的简化，图卷积神经网络（GCN）模型也可以看作是一种空间方法。 图循环神经网络 图神经网络的发展还有一个趋势，那就是在传播步骤使用循环神经网络（RNNs）的门机制，比如GRU、LSTM，以提高图中长期信息传播的有效性。Li等人[2016]提出门控图神经网络（GGNN）它在传播步骤中使用了门循环单元[9]。该模型中，消息函数为\\(\\mathcal{M}_{t}(h_{t,v_{i}}, h_{t,v_{j}}, h_{t,e_{ij}}) = W_{t} h_{t,v_{j}}\\)，聚合方案\\(\\mathcal{A}\\)为sum，更新函数为\\(\\mathcal{U}_{t}(h_{t,v_{i}}, m_{t+1,v_{i}}) = \\operatorname{GRU}(h_{t,v_{i}}, m_{t+1,v_{i}})\\)。因此，消息是由 \\[ m_{t+1,v_{i}} = \\sum_{v_{j} \\in \\mathcal{N}(v_{i})} W_{t} h_{t,v_{j}} \\] 生成的，其中\\(h_{0,v_{j}} = [x_{v_{j}} ; 0]\\)是节点特征用零一直填充到\\(\\text{out_channels} \\times \\text{out_channels}\\)维数得到的初始隐藏状态。接下来，门控循环单元被调用来更新隐藏的嵌入： \\[ h_{t+1,v_{i}} = \\operatorname{GRU}(h_{t,v_{i}}, m_{t+1,v_{i}}). \\] 更新函数的详细过程可展开如下： \\[ z_{t,v_{i}} = \\sigma(W_{z,m} m_{t+1,v_{i}} + W_{z,h} h_{t,v_{i}} + b_{z}), \\] \\[ r_{t,v_{i}} = \\sigma(W_{r,m} m_{t+1,v_{i}} + W_{r,h} h_{t,v_{i}} + b_{r}), \\] \\[ \\tilde{h}_{t+1,v_{i}} = \\tanh(W_{c,m} m_{t+1,v_{i}} + W_{c,h} (r_{t,v_{i}} \\odot h_{t,v_{i}})), \\] \\[ h_{t+1,v_{i}} = (1 - z_{t,v_{i}}) \\odot h_{t,v_{i}} + z_{t,v_{i}} \\odot \\tilde{h}_{t+1,v_{i}}. \\] 正如你所看到的，\\(t+1\\)层的隐藏状态\\(h_{t+1,v_{i}}\\)是之前隐藏状态\\(h_{t,v_{i}}\\)和候选激活\\(\\tilde{h}_{t+1,v_{i}}\\)之间的线性插值。 将节点表示和对应的标签映射到期望输出的输出函数采用如下形式： \\[ \\mathcal{O}(h_{T,v}, h_{0,v}) = \\tanh\\left(\\sum_{v \\in \\mathbb{V}} \\sigma(\\operatorname{NN}_{a}(h_{T,v}, h_{0,v})) \\odot \\tanh(\\operatorname{NN}_{b}(h_{T,v}, h_{0,v}))\\right), \\] 式中，\\(\\sigma(\\operatorname{NN}_{a}(h_{T,v}, h_{0,v}))\\)作为一个软注意机制，决定哪些节点与当前的图级（graph-level）任务相关。\\(\\operatorname{NN}_{a}\\)和\\(\\operatorname{NN}_{b}\\)是两个神经网络，以\\(h_{T,v}\\)和\\(h_{0,v}\\)作为输入，输出实值矢量。另外，\\(\\tanh\\)函数也可以用恒等函数代替。GGNN的实现可以参考PyTorch geometric: GatedGraphConv。 图注意力神经网络 注意机制已成功地应用在许多基于序列的任务，例如机器翻译、机器阅读等等。注意力函数可以描述为将查询和一组键-值对到输出的映射，其中查询、键、值和输出都是矢量[10]。输出以值的加权和的形式计算，其中分配给每个值的权重由查询与相应键的兼容函数计算。可以用下式紧凑地表述注意力机制： \\[ \\operatorname{Attention}(Q,K,V) = \\operatorname{Compatibility}(Q,K)V. \\] 与同等对待一个节点的所有邻居的图卷积神经网络相比，注意机制可以给每个邻居分配不同的注意分数，从而识别出更重要的邻居。所以将注意力机制引入图神经网络可以在某种程度上增强网络的学习能力。Velickovic等人[2017]提出一个图注意力网络（GAT），在图神经网络的传播步骤中并入了注意力机制机制。该算法采用自注意策略，通过有所侧重地关注其邻居来计算每个节点的隐藏状态[11]。 为了将输入特征转换成高阶的特征从而获得足够的表达能力，我们至少需要一个可学习的线性变换。为此，作为初始步骤，对每个节点应用一个共同的变换矩阵为可学习的权值矩阵\\(W \\in \\mathbb{R}^{F' \\times F}\\)的线性变换，将特征矢量投影到更具表现力的空间。然后我们在节点上执行相同的自注意力机制，\\(a : \\mathbb{R}^{F'} \\times \\mathbb{R}^{F} \\to \\mathbb{R}\\)，来计算注意系数： \\[ e_{ij} = a(W h_{v_{i}},W h_{v_{j}}), \\] 这指出了节点\\(v_{j}\\)的特征对节点\\(v_{i}\\)的重要性。为了使系数在不同节点之间易于比较，我们使用softmax函数对系数进行归一化： \\[ \\alpha_{ij} = \\operatorname{softmax}(e_{ij}) = \\frac{\\exp(e_{ij})}{\\sum_{k \\in \\mathcal{N}_{i}} \\exp(e_{ik})}. \\] 在Velickovic等人[2017]的工作中，注意机制\\(a\\)是一个单层的前馈神经网络，由权值向量\\(a \\in \\mathbb{R}^{2F'}\\)参数化，并采用LeakyReLU非线性单元（负输入的斜率为0.2）。完全展开后，由注意机制计算出的系数可表示为: \\[ \\alpha_{i j} = \\frac{\\exp \\left(\\operatorname{ LeakyReLU }\\left(a^{\\text{T}} [W h_{v_i} \\| W h_{v_j}]\\right)\\right)}{\\sum_{k \\in \\mathcal{N}_{i}} \\exp \\left(\\operatorname{ LeakyReLU }\\left(a^{\\text{T}} [W h_{v_i} \\| W h_{v_k}]\\right)\\right)}, \\] 式中，\\(\\|\\)表示连结操作。 然后每个节点的最终输出特性可以通过下式获得： \\[ h_{v_i}^{\\prime} = \\sigma\\left(\\sum_{j \\in \\mathcal{N}_{i}} \\alpha_{i j} W h_{v_j}\\right). \\] 为了稳定自注意的学习过程，扩展至多头注意力机制是有益的。具体的做法就是，采用\\(K\\)个独立的注意机制一起执行上式的变换，然后将它们的聚合。聚合可以是连结，也可以是取平均： \\[ h_{v_i}^{\\prime} = \\|_{k=1}^{K} \\sigma\\left(\\sum_{j \\in \\mathcal{N}_{i}} \\alpha_{i j}^{k} W^{k} h_{v_j}\\right) \\quad \\text{or} \\quad h_{v_i}^{\\prime} = \\sigma\\left(\\frac{1}{K} \\sum_{k=1}^{K} \\sum_{j \\in \\mathcal{N}_{i}} \\alpha_{i j}^{k} W^{k} h_{v_j}\\right). \\] 下图是对多头注意力机制的形象的图形描述，需要注意的是它有个自连接线，考虑了其本身的重要性。 GAT的实现可以参考PyTorch geometric: GATConv。 Velickovic等人[2018]提出的这种注意架构有几个性质：（1）节点近邻对的计算是并行的，因此操作是高效的；（2）可以对不同程度的节点进行处理，并为其相邻节点分配相应的权重；（3）易应用于归纳学习问题。因此，在半监督节点分类、链路预测等任务中，GAT的性能优于GCN。 结语 此贴以消息传递的基本概念开始，介绍了图神经网络的基本框架，旨在为读者提供一个一般性的概述。然后介绍三个不同的图神经网络变体：图卷积神经网络、图循环神经网络、图注意力神经网络。这些变体是各种不同深度学习技术在图上的推广，是机器学习在非欧流形的普及。目前，图神经网络在结构场景（物理、化学、知识图）、非结构场景（图像、文本）和其他场景（生成模型、组合优化）中都有广泛的应用，并取得了不错的进展[1]。希望此文对想要学习图神经网络的读者有所帮助。 参考文献 Liu, Zhiyuan. Introduction to Graph Neural Networks. Morgan &amp; Claypool Publishers, 2020, ISBN:978-1-68173-767-6. Hamilton, William. Graph Representation Learning. Morgan &amp; Claypool Publishers, 2020, ISBN:978-1-68173-963-2. Gilmer, Justin, et al. “Neural Message Passing for Quantum Chemistry.”, 2017, arXiv:1704.01212. Scarselli, F., et al. “The Graph Neural Network Model.” Transactions on Neural Networks, vol. 20, no. 1, Jan. 2009, pp. 61–80. DOI:10.1109/tnn.2008.2005605. Bruna, Joan, et al. “Spectral Networks and Locally Connected Networks on Graphs.” , 2013, arXiv:1312.6203. Hammond, David K., et al. “Wavelets on graphs via spectral graph theory.” Applied and Computational Harmonic Analysis, vol. 30, no. 2, Mar. 2011, pp. 129–150. DOI:10.1016/j.acha.2010.04.005. Defferrard, Michaël, et al. “Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering.”, 2016, arXiv:1606.09375. Kipf, Thomas N. and Max Welling. “Semi-Supervised Classification with Graph Convolutional Networks.”, 2016, arXiv:1609.02907. Li, Yujia, et al. “Gated Graph Sequence Neural Networks.”, 2015, arXiv:1511.05493. Vaswani, Ashish, et al. “Attention Is All You Need.”, 2017, arXiv:1706.03762. Veličković, Petar, et al. “Graph Attention Networks.”, 2017, arXiv:1710.10903. Wu, Zonghan, et al. “A Comprehensive Survey on Graph Neural Networks.” Transactions on Neural Networks and Learning Systems, 2020, pp. 1–21. DOI:10.1109/tnnls.2020.2978386.","link":"/blog/2020/11/24/introduction-to-graph-neural-networks/"},{"title":"Loss Functions","text":"One of the key ingredients of supervised machine learning algorithms is a defined objective function that is to be optimized during the learning process. This objective function is often a loss function that we want to minimize. This post shows some key concepts of the loss functions. Terminology Loss function: Often used synonymously with a cost function. Sometimes the loss function is also called an error function. In some literature, the term \"loss\" refers to the loss measured for a single data point, and the cost is a measurement that computes the loss (average or summed) over the entire dataset. Loss Functions for Cassification Overview Loss Functions for Classification Essentials Binary cross-entropy for binary classifications (with a single output unit) Categorical cross-entropy for multiclass classifications one-hot encoded ground truth labels Sparse categorical cross-entropy for multiclass classifications integer (sparse) ground truth labels Emphasis Please note that computing the cross-entropy loss by providing the logits, and not the class-membership probabilities, is usually preferred due to numerical stability reasons. If we provide logits as inputs to the loss function and set from_logits=True , the respective TensorFlow function uses a more efficient implementation to compute the loss and derivative of the loss with respect to the weights. This is possible since certain mathematical terms cancel and thus don't have to be computed explicitly when providing logits as inputs. Usages In:12345678910111213141516171819202122232425262728293031323334353637383940####### Binary Crossentropybce_probas = tf.keras.losses.BinaryCrossentropy(from_logits=False)bce_logits = tf.keras.losses.BinaryCrossentropy(from_logits=True)logits = tf.constant([0.8])probas = tf.keras.activations.sigmoid(logits)tf.print( 'BCE (w Probas): {:.4f}'.format( bce_probas(y_true=[1], y_pred=probas)), '(w Logits): {:.4f}'.format( bce_logits(y_true=[1], y_pred=logits)))####### Categorical Crossentropycce_probas = tf.keras.losses.CategoricalCrossentropy( from_logits=False)cce_logits = tf.keras.losses.CategoricalCrossentropy( from_logits=True)logits = tf.constant([[1.5, 0.8, 2.1]])probas = tf.keras.activations.softmax(logits)tf.print( 'CCE (w Probas): {:.4f}'.format( cce_probas(y_true=[0, 0, 1], y_pred=probas)), '(w Logits): {:.4f}'.format( cce_logits(y_true=[0, 0, 1], y_pred=logits)))####### Sparse Categorical Crossentropysp_cce_probas = tf.keras.losses.SparseCategoricalCrossentropy( from_logits=False)sp_cce_logits = tf.keras.losses.SparseCategoricalCrossentropy( from_logits=True)tf.print( 'Sparse CCE (w Probas): {:.4f}'.format( sp_cce_probas(y_true=[2], y_pred=probas)), '(w Logits): {:.4f}'.format( sp_cce_logits(y_true=[2], y_pred=logits))) Out:123BCE (w Probas): 0.3711 (w Logits): 0.3711CCE (w Probas): 0.5996 (w Logits): 0.5996Sparse CCE (w Probas): 0.5996 (w Logits): 0.5996 References S. Raschka and V. Mirjalili. Python Machine Learning: Machine Learning and Deep Learning with Python, scikit-learn, and TensorFlow 2. Packt Publishing Ltd, Birmingham, third edition, 2019.","link":"/blog/2020/05/26/loss-functions/"},{"title":"Machine Learning-Based Scoring Functions","text":"[To be revised] Random Forest-Based Scoring Functions RF-Score-VS Applications: virtual screening RF-Score-VS might not have good capability to discover ligands for completely novel targets. SIEVE-Score Applications: virtual screening Performance: better than Glide, Dock, AutoDock Vina, RF-Score-VS, interaction fingerprints (SPLIF, PLIF, FCFP4). \\(\\Delta_{\\text{vina}} \\text{RF}_{20}\\) trained based on three datasets, including the PDBbind-v2014 refined set, the native poses in the CSAR decoy dataset and the weak binding structures in the PDBbind-v2014 general set optimizing the correction term with ML algorithms might be a promising way to efficiently improve the performance and robustness of SFs Support Vector Machines-Based Scoring Functions","link":"/blog/2020/09/18/machine-learning-based-scoring-functions/"},{"title":"Matplotlib 3.1 Cheat Sheet","text":"Matplotlib is frequently used by me for plotting figures. Lately, a cheat sheet was created in the efforts of Nicolas P. Rougier, which offers great convenience for reference. The following shows the PNG formatted cheat sheet. Click here to the git repository for more information. Matplotlib Cheat Sheet","link":"/blog/2020/01/27/matplotlib-3-1-cheat-sheet/"},{"title":"Message Passing Neural Networks","text":"[To be revised] A variety of graph neural networks can be expressed as a neighborhood aggregation or message passing scheme. GNNs are a general neural network architecture defined according to a graph structure \\(\\mathbb{G} = (\\mathbb{V}; \\mathbb{E})\\). GNNs map graphs to outputs via two steps. First, there is a propagation step that computes node representations for each node; second, an output model \\(\\hat{y}_{v} = g(hv; lv)\\) maps from node representations and corresponding labels to an output ov for each \\(v \\in \\mathbb{V}\\). The forward pass has two phases, a message passing phase and a readout phase. The message passing phase runs for \\(T\\) time steps and is defined in terms of message functions \\(\\mathcal{M}_{t}\\) and vertex update functions \\(\\mathcal{U}_{t}\\). During the message passing phase, hidden states \\(h_{t+1,v_{i}}\\) at each node in the graph are updated based on messages \\(m_{t+1,v_{i}}\\) according to \\[ m_{t+1,v_{i}} = \\mathcal{A}(\\{\\mathcal{M}_{t}(h_{t,v_{i}}, h_{t,v_{j}}, h_{t,e_{ij}}) | v_{j} \\in \\mathcal{N}(v_{i})\\}) \\] \\[ h_{t+1,v_{i}} = \\mathcal{U}_{t}(h_{t,v_{i}}, m_{t+1,v_{i}}) \\] where \\(\\mathcal{A}\\) is the aggregation scheme, e.g., sum, mean or max, and \\(\\mathcal{N}(v_{i})\\) denotes the neighbors of \\(v_{i}\\) in graph \\(G\\). When there are multiple edge types, we must define multiple message functions, \\(\\mathcal{M}_{t,\\operatorname{type}(e_{ij})}\\), which is the message function at layer \\(t\\) for edge type \\(\\operatorname{type}(e_{ij})\\). The readout phase computes a feature vector for the whole graph using some readout function \\(\\mathcal{R}\\) according to \\[ \\hat{y} = \\mathcal{O}(\\{h_{v} | v \\in \\mathbb{V}\\}) \\] Graph Convolutional Networks Spectral Methods GCN \\[ m_{t+1,v_{i}} = \\sum_{v_{j} \\in \\mathcal{N}(v_{i}) \\cup v_{i}} \\frac{W_{t} h_{t,v_{j}}}{\\sqrt{\\operatorname{deg}(v_{i}) \\operatorname{deg}(v_{j})}} \\] \\[ h_{t+1,v_{i}} = \\operatorname{ReLU}(W_{t}^{\\prime} m_{t+1,v_{i}}) \\] Graph Recurrent Networks Gated Graph Neural Networks Li et al. (2016) propose the GGNN which uses the Gate Recurrent Units (GRU) in the propagation step. In this model, the message functions is \\(\\mathcal{M}_{t}(h_{t,v_{i}}, h_{t,v_{j}}, h_{t,e_{ij}}) = W_{t} h_{t,v_{j}}\\), the aggregation scheme \\(\\mathcal{A}\\) is sum and the update functions is \\(\\mathcal{U}_{t}(h_{t,v_{i}}, m_{t+1,v_{i}}) = \\operatorname{GRU}(h_{t,v_{i}}, m_{t+1,v_{i}})\\). Thus, the massages is generated by \\[ m_{t+1,v_{i}} = \\sum_{v_{j} \\in \\mathcal{N}(v_{i})} W_{t} h_{t,v_{j}} \\] where \\(h_{0,v_{j}} = [x_{v_{j}} ; 0]\\) is padded up to \\(\\text{out_channels} \\times \\text{out_channels}\\) dimension with zeros. Next the gated recurrent unit is invoked to update the hidden embeddings: \\[ h_{t+1,v_{i}} = \\operatorname{GRU}(h_{t,v_{i}}, m_{t+1,v_{i}}) \\] The detailed procedures in the \\(\\operatorname{GRU}\\) update functions can be expanded as follows: \\[ z_{t,v_{i}} = \\sigma(W_{z,m} m_{t+1,v_{i}} + W_{z,h} h_{t,v_{i}} + b_{z}) \\] \\[ r_{t,v_{i}} = \\sigma(W_{r,m} m_{t+1,v_{i}} + W_{r,h} h_{t,v_{i}} + b_{r}) \\] \\[ \\tilde{h}_{t+1,v_{i}} = \\tanh(W_{c,m} m_{t+1,v_{i}} + W_{c,h} (r_{t,v_{i}} \\odot h_{t,v_{i}})) \\] \\[ h_{t+1,v_{i}} = (1 - z_{t,v_{i}}) \\odot h_{t,v_{i}} + z_{t,v_{i}} \\odot \\tilde{h}_{t+1,v_{i}} \\] And as you can see, the hidden state \\(h_{t+1,v_{i}}\\) of \\(t+1\\) layer is a linear interpolation between the previous hidden state \\(h_{t,v_{i}}\\) and the candidate activation \\(\\tilde{h}_{t+1,v_{i}}\\). The output function mapping node representations and corresponding labels to an the desired output adopts this form: \\[ \\mathcal{O}(h_{T,v}, h_{0,v}) = \\tanh\\left(\\sum_{v \\in \\mathbb{V}} \\sigma(\\operatorname{NN}_{a}(h_{T,v}, h_{0,v})) \\odot \\tanh(\\operatorname{NN}_{b}(h_{T,v}, h_{0,v}))\\right) \\] Edge Convolution \\[ m_{t+1,v_{i}} = \\max_{v_{j} \\in \\mathcal{N}(v_{i})} \\operatorname{MLP}([h_{t,v_{i}}; h_{t,v_{j}} - h_{t,v_{i}}]) \\] Convolutional Neural Networks \\[ M_{t}(h_{t,v_{i}}, h_{t,v_{j}}, x_{e_{ij}}) = [h_{t,v_{j}}; x_{e_{ij}}] \\] \\[ U_{t}(h_{t,v_{i}}, m_{t+1,v_{i}}) = \\sigma(H_{t,\\operatorname{deg}(v_{i})} m_{t+1,v_{i}}) \\] \\[ R(\\{h_{v} \\mid v \\in G\\}) = f\\left(\\sum_{t,i} \\operatorname{softmax}(W_{t} h_{t,v_{i}})\\right) \\] Gated Graph Neural Networks Interaction Networks Molecular Graph Convolutions \\[ M_{t}(h_{t,v_{i}}, h_{t,v_{j}}, h_{t,e_{ij}}) = h_{t,e_{ij}} \\] \\[ U_{t}(h_{t,v_{i}}, m_{t+1,v_{i}}) = \\alpha(W_{1} [\\alpha(W_{0} h_{t,v_{i}}); m_{t+1,v_{i}}]) \\] \\[ h_{t+1,e_{ij}} = U_{t}^{\\prime}(h_{t,e_{ij}}, h_{t,v_{i}}, h_{t,v_{j}}) = \\alpha(W_{4} [\\alpha(W_{2} h_{t,e_{ij}}); \\alpha(W_{3} [h_{t,v_{i}}; h_{t,v_{j}}])]) \\] Laplacian Based Methods GCN \\[ M_{t}(h_{t,v_{i}}, h_{t,v_{j}}) = C_{t,ij} h_{t,v_{j}} \\] \\[ U_{t}(h_{t,v_{i}}, m_{t+1,v_{i}}) = \\operatorname{ReLU}(W_{t} m_{t+1,v_{i}}) \\] 0 1 0 1","link":"/blog/2020/10/09/message-passing-neural-networks/"},{"title":"Music Score: Season Song","text":"This post deposits the music score of \"Season Song\" for convenience of reference. Season Song Music Score Season Song LilyPond Code The PNG-type score is generated by LilyPond. 123456789101112131415161718\\version &quot;2.20.0&quot;\\header { title = &quot;四季の歌&quot;}\\score { \\new Staff { \\time 4/4 \\tempo &quot;Moderato&quot; e''4( e''8 d''8 c''8 d''8 c''8 b'8 | a'4 a'4 a'2) | f''4( f''8 e''8 d''8 c''8 d''8 f''8 | e''1) | \\break f''4( f''8 e''8 d''4 d''8 f''8 | e''4 e''8 c''8 a'4 c''4) | b'4( e''4 d''8 c''8 b'8 c''8 | a'1) | \\break f''4( f''8 e''8 d''4 d''8 f''8 | e''4 e''8 c''8 a'4 c''4) | b'4( e''4 d''8 c''8 b'8 c''8 | a'1) \\bar &quot;|.&quot; } \\layout { indent = 0\\mm }} And the PDF version is also compiled, which can be obtained here (code: gceb).","link":"/blog/2020/11/04/music-score-season-song/"},{"title":"Parallel Programming","text":"The goal of this post is to provide an overview of introductory concepts and terminologies in parallel computing. We will get to know about the two most important parallel architectures: distributed memory systems and shared memory systems. This post is only a crude summary, which is badly organized and not complete (without CUDA and unified parallel C++). Of course, It will be improved and supplemented in the future. Introduction What's CPU? CPU(s) = Thread(s) per core X core(s) per socket X socket(s) 12345$ lscpu | grep -E '^Thread|^Core|^Socket|^CPU\\('CPU(s): 4Thread(s) per core: 1Core(s) per socket: 4Socket(s): 1 concurrency task switching hardware concurrency (parallel) architectures Flynn’s taxonomy SISD Single instruction stream; single data stream. SIMD Single instruction stream; multiple data stream. MISD Multiple instruction stream; single data stream. MIMD Multiple instruction stream; multiple data stream. memory structure share memory distributed memory hybrid memory Multithreading A simple Hello Concurrent World program 12345678910111213#include &lt;iostream&gt;#include &lt;thread&gt;void hello(){ std::cout &lt;&lt; &quot;Hello Concurrent World\\n&quot;;}int main(){ std::thread t(hello); t.join();} Message Passing Interface The most common programming model for distributed-memory systems is message passing. The Message Passing Interface (MPI) is established as a de facto standard as it is based on the consensus of the MPI Forum. Introduction to MPI history Before the 1990s: no uniform standard 1994: MPI-1 2015: MPI-3.1 (up-to-date) supported languages Fortran, C, and C++. implementations open-source: OpenMPI, MPICH. commercial: Intel, IBM, HP. MPI program compile 1mpic++ -o hello hello.cpp execute 1mpirun -np 4 ./hello The number of processes remains constant throughout the whole execution. Processes are mapped to different nodes as specified by the user in a configuration file. Basic Concepts We start our journey into MPI programming with a version of the popular Hello World. 123456789101112131415161718192021#include &quot;mpi.h&quot;int main (int argc, char *argv[]){ // Initialize MPI MPI::Init(argc,argv); // Get the number of processes int numP = MPI::COMM_WORLD.Get_size(); // Get the ID of the process int myId = MPI::COMM_WORLD.Get_rank(); // Every process prints Hello std::cout &lt;&lt; &quot;Process &quot; &lt;&lt; myId &lt;&lt; &quot; of &quot; &lt;&lt; numP &lt;&lt; &quot;: Hello, world!&quot; &lt;&lt; std::endl; // Terminate MPI MPI::Finalize(); return 0;} key concepts: In the main() function all processes are completely independent until the MPI initialization (MPI::Init()). From this point the processes can collaborate, send/receive messages or synchronize until reaching MPI::Finalize(). MPI uses communicator objects to define which collection of processes may communicate with each other. MPI::COMM_WORLD is a predefined object that consists of all the MPI processes launched during our execution. a possible outcome 1234Process 3 of 4: Hello, world!Process 0 of 4: Hello, world!Process 1 of 4: Hello, world!Process 2 of 4: Hello, world! Point-to-Point Communication The traditional communication scheme in MPI is two-sided, where both source and destination processes must indicate in the code that they are part of the communication. blocking communication 12void Send(const void* buf, int count, const Datatype&amp; datatype, int dest, int tag);void Recv(void* buf, int count, const Datatype&amp; datatype, int source, int tag); NOT EXACTLY BLOCKING: The formal Send() definition states that it returns only after the application buffer in the sending task is free for reuse. NOTICE: Blocking communication could lead to deadlocks among processes. nonblocking communication 12MPI::Request Isend(const void* buf, int count, const Datatype&amp; datatype, int dest, int tag)MPI::Request Irecv(void* buf, int count, const Datatype&amp; datatype, int source, int tag) RETURN TYPE: The object of the class MPI::Request contains information about the state of the messages. SYNCRONIZATION: MPI::Request::Wait(), MPI::Request::Test(). Collectives MPI provides a set of routines for communication patterns that involve all the processes of a certain communicator, so-called collectives. Barrier() is the simplest collective available in MPI that synchronizes the processes of a specified communicator: no process can continue until all have reached the barrier. Bcast() send a message from one process to all other processes in the specified communicator. 1void Bcast(void* buffer, int count, const MPI::Datatype&amp; datatype, int root); Reduce(): reduce to a value. 12void Reduce(const void* sendbuf, void* recvbuf, int count, const MPI::Datatype&amp; datatype, const MPI::Op&amp; op, int root); Allreduce(): Combination of reduction and a broadcast so that the output is available for all processes. Scatter(): Split a block of data available in a root process and send different fragments to each process. Gather(): Send data from different processes and aggregate it in a root process. Allgather(): Similar to Gather() but the output is aggregated in buffers of all the processes. Alltoall(): All processes scatter data to all processes. Furthermore, there exist variants of these routines for nonblocking communications Boost.MPI Boost.MPI is not a completely new parallel programming library. Rather, it is a C++-friendly interface to the standard Message Passing Interface (MPI). better supports modern C++ development styles complete support for user-defined data types and C++ Standard Library types, arbitrary function objects for collective algorithms the use of modern C++ library techniques to maintain maximal efficiency Tutorial 123456789101112#include &lt;iostream&gt;#include &lt;boost/mpi.hpp&gt;namespace mpi = boost::mpi;int main(){ mpi::environment env; mpi::communicator world; std::cout &lt;&lt; &quot;I am process &quot; &lt;&lt; world.rank() &lt;&lt; &quot; of &quot; &lt;&lt; world.size() &lt;&lt; &quot;.&quot; &lt;&lt; std::endl; return 0;} Unified Parallel Partitioned Global Address Space (PGAS) approaches are gaining attention for programming modern multi-core CPU clusters. They feature a hybrid memory abstraction: distributed memory is viewed as a shared memory that is partitioned among nodes in order to simplify programming.","link":"/blog/2020/10/09/parallel-programming/"},{"title":"Practical Array Manipulations in Python","text":"Mastering ways to manipulate arrays is necessary to make them compatible for input to a model or an operation. In this post, you will learn how to manipulate arrays to meet the requirements that always be encountered in the field of machine learning. Reshape a 1-D array of \\(n\\) elements to a 2-D \\(n \\times 1\\) array NumPy In: 12345678import numpy as npa = np.linspace(0, 4, num=5)print(a)print('-' * 25)print(a.reshape(-1, 1))print('-' * 25)print(a[:, np.newaxis]) # '...' for all dimensions Out: 12345678910111213[0. 1. 2. 3. 4.]-------------------------[[0.] [1.] [2.] [3.] [4.]]-------------------------[[0.] [1.] [2.] [3.] [4.]] TensorFlow In: 123456789import numpy as npimport tensorflow as tfa = np.linspace(0, 4, num=5)print(a)print('-' * 25)print(a[:, tf.newaxis])print('-' * 25)print(tf.expand_dims(a, axis=1)) Out: 1234567891011121314[0. 1. 2. 3. 4.]-------------------------[[0.] [1.] [2.] [3.] [4.]]-------------------------tf.Tensor([[0.] [1.] [2.] [3.] [4.]], shape=(5, 1), dtype=float64) PyTorch In: 12345678910import torcha = torch.linspace(0, 4, 5)print(a)print('-' * 25)print(a.unsqueeze(0))print('-' * 25)print(a.view(1, -1))print('-' * 25)print(a.reshape(1, -1)) Out: 1234567tensor([0., 1., 2., 3., 4.])-------------------------tensor([[0., 1., 2., 3., 4.]])-------------------------tensor([[0., 1., 2., 3., 4.]])-------------------------tensor([[0., 1., 2., 3., 4.]]) Stack arrays in sequence horizontally NumPy In: 123456789import numpy as npa = np.linspace(0, 4, num=5)b = np.linspace(5, 9, num=5)print(a, b)print('-' * 25)print(np.hstack((a.reshape(-1, 1), b.reshape(-1, 1))))print('-' * 25)print(np.c_[a, b]) Out: 12345678910111213[0. 1. 2. 3. 4.] [5. 6. 7. 8. 9.]-------------------------[[0. 5.] [1. 6.] [2. 7.] [3. 8.] [4. 9.]]-------------------------[[0. 5.] [1. 6.] [2. 7.] [3. 8.] [4. 9.]] Squeeze unnecessary dimensions NumPy In: 123456import numpy as npa = np.zeros((1, 2, 1, 4, 1))print(a)print('-' * 25)print(a[0, :, 0, :, 0]) Out: 12345678910111213[[[[[0.] [0.] [0.] [0.]]] [[[0.] [0.] [0.] [0.]]]]]-------------------------[[0. 0. 0. 0.] [0. 0. 0. 0.]] TensorFlow In: 123456import tensorflow as tft = tf.zeros((1, 2, 1, 4, 1))print(t)print('-' * 25)print(tf.squeeze(t, axis=(0, 2, 4))) Out: 123456789101112131415tf.Tensor([[[[[0.] [0.] [0.] [0.]]] [[[0.] [0.] [0.] [0.]]]]], shape=(1, 2, 1, 4, 1), dtype=float32)-------------------------tf.Tensor([[0. 0. 0. 0.] [0. 0. 0. 0.]], shape=(2, 4), dtype=float32)","link":"/blog/2020/04/06/practical-array-manipulations-in-python/"},{"title":"Prepare Structures for Use in Computation","text":"Successful structure-based modeling projects demand not only accurate software, but accurate starting structures as well. Left untreated, common problems with experimentally-derived structures can lead to wasted time and resources. Protein Preparation Protein Preparation Wizard (Schrödinger) An easy-to-use tool for correcting common structural problems and creating reliable, all-atom protein models. Usage Guide: Loading and Preparing a Protein Structure Relevant Article: DOI 10.1007/s10822-013-9644-8 Rosetta@home (Baker lab) How to prepare structures for use in Rosetta Discovery Studio (BIOVIA) preparing a pdb file Ligand Preparation To be continued.","link":"/blog/2020/01/12/prepare-structures-for-use-in-computation/"},{"title":"Python Package Source and Its Mirrors","text":"The Python packages are commonly deposited in the PyPI and Anaconda Cloud. However, it is usually cumbersome to access these repositories in China. In this case, we may turn to the native mirrors for faster accession. Here shows the available mirrors and the ways to configure it. PyPI Packages Default Source PyPI (default) 1https://pypi.python.org/simple/ Available Mirrors Aliyun (recommended) 1http://mirrors.aliyun.com/pypi/simple/ Tsinghua 1https://pypi.tuna.tsinghua.edu.cn/simple/ USTC 1http://pypi.mirrors.ustc.edu.cn/simple/ Configuration When using it temporarily, you can just invoke the following command. 1$ pip install -i &lt;mirror&gt; &lt;package&gt; Example: 1$ pip install -i http://mirrors.aliyun.com/pypi/simple/ numpy To set it as default, you should configure it as follows. 1$ pip config set global.index-url http://mirrors.aliyun.com/pypi/simple/ This will modify the ~/.config/pip/pip.conf file to keep your setting permanently. 12[global]index-url = https://mirrors.aliyun.com/pypi/simple/ Conda Packages Default Source Anaconda Cloud (default) Available Mirrors Tsinghua (recommended) Configuration Creating a ~/usr/local/anaconda3/.condarc file and write 1234567891011121314151617channels: - defaultsshow_channel_urls: truechannel_alias: https://mirrors.tuna.tsinghua.edu.cn/anacondadefault_channels: - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/pro - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2custom_channels: conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud simpleitk: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud","link":"/blog/2020/02/29/python-package-source-and-its-mirrors/"},{"title":"RDKit Uasge Instances","text":"This is part of assignments from the course of artificial intelligence and chemical information for RDKit exercises, including calculation of conformations and fingerprints. Conformations of Cyclohexane A1. sample 10 conformations of cyclohexane, then use MMFF94 force field to minimize these conformers. 1234567891011121314from rdkit import Chemfrom rdkit.Chem import AllChemm = Chem.MolFromSmiles('C1CCCCC1')m.SetProp(&quot;_Name&quot;,&quot;cyclohexane&quot;)m = Chem.AddHs(m)cids = AllChem.EmbedMultipleConfs(m, numConfs=10)res = AllChem.MMFFOptimizeMoleculeConfs(m) print(&quot;optimization results:\\n&quot;, res)w = Chem.SDWriter('chx_cfs.sdf')for cid in cids: w.write(m, confId=cid)w.close() 12optimization results: [(0, 2.368812250031951), (0, -3.5609335464164986), (0, -3.5609335469816084), (0, 2.3688122495973722), (0, -3.5609335428100186), (0, -3.560933551197448), (0, 2.368812250871666), (0, -3.560933544925278), (0, -3.5609335512000726), (0, 2.3688122506104126)] Fingerprinting and Molecular Similarity A2. calculate fingerprints of molecules in mols.csv, and after that compare the molecular similarity of the first molecule to all others. 123456789101112131415161718192021222324252627import numpy as npimport pandas as pdfrom rdkit import Chemfrom rdkit.Chem import MACCSkeysfrom rdkit.Chem.AtomPairs import Pairs, Torsionsfrom rdkit import DataStructs# input smiles datams = [Chem.MolFromSmiles(m) for m in pd.read_csv(&quot;data/mols.csv&quot;)[&quot;SMILES&quot;]]# calculate fingerprintsfps = {} fps['topol'] = [Chem.RDKFingerprint(m) for m in ms]fps['maccs'] = [MACCSkeys.GenMACCSKeys(m) for m in ms]fps['apair'] = [Pairs.GetAtomPairFingerprint(m) for m in ms]fps['torsi'] = [Torsions.GetTopologicalTorsionFingerprintAsIntVect(m) for m in ms]fps['morga'] = [AllChem.GetMorganFingerprint(m, 2) for m in ms]# find the most similar mol to the 1st mol from the rest 99 molssim, mol = {}, {}for key, value in fps.items(): sim[key] = DataStructs.BulkTanimotoSimilarity(value[0], value[1:]) mol[key] = ms[np.argmax(sim[key]) + 1]img = Chem.Draw.MolsToGridImage([ms[0]] + list(mol.values()), molsPerRow=3, subImgSize=(400, 300), legends=['query'] + list(mol.keys()))img.save('images/sim_mol.png') Similar Molecules 12# calculate the full correlation matrixpd.DataFrame(sim).corr() topol maccs apair torsi morga topol 1.000000 0.443202 0.292626 0.425684 0.172240 maccs 0.443202 1.000000 0.430594 0.185562 0.347820 apair 0.292626 0.430594 1.000000 0.512514 0.834419 torsi 0.425684 0.185562 0.512514 1.000000 0.611803 morga 0.172240 0.347820 0.834419 0.611803 1.000000 The jupyter version of the above codes can be found here.","link":"/blog/2020/03/31/rdkit-uasge-instances/"},{"title":"Scoring Functions: A Comprehensive Guide","text":"[To be revised] Scoring functions are widely applied in structure-based drug design for evaluating protein−ligand interactions, which make various approximations in order to provide a compromise between speed and accuracy instead of attempting to account for the physics in a protein−ligand binding process with a high-level theory. Scoring functions are thus particularly suitable for high-throughput tasks, such as molecular docking, virtual screening, library design, and so on. This post summarizes the basics of scoring functions, and thus gives readers a fundamental and comprehensive guide. The real challenge in protein−ligand binding affinity prediction lies in polar interactions and associated desolvation effect. Nonadditive features observed among high-affinity protein−ligand complexes also need attention. Databases and Benchmarks Databases PDBbind CONTAIN: three-dimensional structures and binding affinity data PDBbind database provides a comprehensive collection of the experimentally measured binding affinity data for all types of biomolecular complexes deposited in the Protein Data Bank (PDB). ChEMBL perhaps the largest public resource of protein−ligand binding data BindingDB PubChem Binding MOAD Benchmarks A good benchmark should have two essential qualities: high-quality three-dimensional structures and reliable binding data diverse protein-ligand complexes CASF Project FULL NAME: comparative assessment of scoring functions (CASF) project CONTAIN: three cornerstones, i.e., a set of protein−ligand complexes as the test set, a panel of popular scoring functions, and corresponding evaluation methods CASF-2007 VERSION: 2007, the first completed study DATASET: 195 diverse protein-ligand complexes with high-resolution crystal structures and reliable binding constants (the PDBbind core set) CASF-2013 VERSION: 2013 DATASET: 195 protein−ligand complexes (65 clusters, 3 members per cluster), including PDB codes, experimental binding data, protein names, and EC numbers. Astex Diverse Set CSAR exercise The community structure–activity resource exercise CSAR-NRC HiQ data set for blinded prediction tests D3R Grand Challenge DUD/DUD-E Directory of Useful Decoys (DUD), Database of Useful Decoys-Enhanced (DUD-E) screening-power-test benchmark specially designed to develop SFs for VS MUV Maximum Unbiased Validation specially designed to develop SFs for VS Scoring Functions Physics-Based Methods Force-Field-Based Scoring Functions In general, an FF-based SF can always be described as a weighted sum of several non-bonded (such as van der Waals, electrostatic and hydrogen bonding) interaction terms computed by a given FF, and the weights for all terms are set to 1 by default. Empirical Scoring Functions Buried Solvent-Accessible Surface Area This scoring function uses a single descriptor, i.e., the buried solvent-accessible surface area of the ligand molecule upon binding (BSASA), which is an estimation of the size of the protein−ligand binding interface. BSASA is a descriptor of the nonpolar factors in protein−ligand interactions, which is nondiscriminative in nature. good scoring/ranking power, bad docking/screening power. GlideScore GlideScore-XP is arguably the most sophisticated empirical scoring function at present. GlideScore-XP is designed with an emphasis on recognizing the diversity in protein binding sites by rewarding or penalizing certain interaction patterns. Autodock Vina, GlideScore, ChemScore and X-Score Knowledge-Based Potentials In these SFs, the 3D coordinates of a large set of protein–ligand complexes are regarded as a knowledge base, and then the relative occurrence frequencies of some features, such as atom–atom pairwise contacts, are calculated. RF-Score-v1 intrinsically belongs to knowledge-based SFs, in which the number of occurrences of a certain protein–ligand atom type pair within a given distance range was simply counted and used as the features. PMF, DrugScore, and ASP SPA-SE score function Descriptor-Based Scoring Functions Depending on whether the three-dimensional (3D) structure of a target is used or not, VS approaches can be classified into two major categories: ligand-based virtual screening (LBVS) and structure-based virtual screening (SBVS). [10.1002/wcms.1429] ligand-based methods more established structure-based models AtomNet Input: 3D grid structure data Model: convolutional neural network for classification Output: 1 if the ligand is active and 0 otherwise Ragoza's model Note: similar to AtomNet; trained to perform two independent classification tasks: activity and pose prediction Gomes's model Input: ... Model: regression Output: the energy gap between a bounded protein–ligand complex and an unbounded state. Pafnucy Input: 3D grid structure data (19 features were used to describe an atom) Model: convolutional neural network for regression Output: binding affinity value evaluate our model with the CASF-2013 ‘scoring power’ benchmark develop a model that was not sensitive to ligand–receptor complex orientation. every structure was presented to the network in 24 different orientations not always in high quality Hybridized Methods The \\(\\Delta_{\\text{vina}} \\text{RF}_{20}\\) Scoring Function Category: \\(\\Delta\\)-machine learning Resources: web, git. \\(n\\) sample \\(d\\) features (\\(m\\) total) \\(k\\) trees \\[ y_{\\text{pred}} = \\frac{1}{k} \\sum_{i = 1}^{k} T_{i} (\\bold{X}) \\] Details of the Random Forests Model Model Input: \\(\\{(X, \\Delta \\text{p}K_{\\text{d}})\\}\\) The feature vector \\(X\\) includes 20 descriptors, which are calculated based on the corresponding protein-ligand structure. There are 10 terms from the AutoDock Vina source code and 10 terms related to buried solvent-accessible surface area (bSASA). 1234567Features (20)├── AutoDock Vina features (10) └── interaction terms (5) └── ligand-dependent terms (5)└── bSASA features (10) └── pharmacophore-based bSASA (9) └── total bSASA (1) The features are selected by employing a strategy that includes both feature selection and aggregation. Physically meaningful feature categories: entropy-related, solvation-related. model: \\(d = 4\\) features, \\(k = 500\\) trees Training Dataset: 3336 experimental crystal structures and 3322 computer generated decoy structures Testing Scoring Power Testing sets: CASF-2013 and CASF-2007 benchmark sets Evaluation methods: Pearson’s correlation coefficient (\\(R\\)), standard deviation (SD) Ranking Power Testing sets: CASF-2013 and CASF-2007 benchmark sets Evaluation methods: two levels of success, namely high-level and low-level Docking Power Testing sets: 100 decoy binding poses Evaluation methods: success rate Screening Power Testing sets: complexes by cross docking 195 ligands on 65 target proteins (50 poses for each) Evaluation methods: metrics-enhancement factors, success rates Molecular Docking Programs DOCK, AutoDock, GOLD, Glide, and Surflex-Dock. FlexX Some Notes DrugBank the upper limit of ligand efficiency: −1.75 kcal/mol per atom (“soft limit”: −0.83 kcal/mol per atom) -1.94 UCSF Chimera Poseview https://github.com/atomistic-machine-learning/schnetpack AutoDock Vina Appendices {CADD: {VS: {LBVS, SBVS: {molecular docking}}}} Chemical Equilibrium \\[ \\Delta G_{\\text{binding}} = − R T \\ln K_{\\text{a}} = - 8.31446 \\times 298.15 \\times \\ln 10 \\times \\log K_{\\text{a}} = −1.3642 \\log K_{\\text{a}} \\ \\text{(kcal/mol)} \\] References","link":"/blog/2020/09/08/scoring-functions-a-comprehensive-guide/"},{"title":"T-Rex: Chrome Dino Game","text":"Google Chrome includes an endless runner Dinosaur game which appears in the absense of internet connection. If you are unable to get the No Internet page, open a new tab and type chrome://dino and press enter. Playing Space Bar / Up: Jump (also to start the game) Down: Duck (pterodactyls appear after 450 points) Alt: Pause The game enters a black background mode after every multiple of 700 points for the next 100 points. Hacking Open Chrome Console Make sure you are on the No Internet Connection page. Right click anywhere on the page and select Inspect. Go to Console tab. This is where we will enter the commands to tweak the game. T-Rex Hacking Tweaking Speed Type the following command in Console and press enter. You can use any other speed in place of 1000. 1Runner.instance_.setSpeed(1000) Immortality After every command press enter. All the commands are case-sensitive. We store the original game over function in a variable: 1var original = Runner.prototype.gameOver Then, we make the game over function empty: 1Runner.prototype.gameOver = function(){} Stopping the game after using Immortality When you want to stop the game, Revert back to the original game over function: 1Runner.prototype.gameOver = original Setting the current score Let’s set the score to 12345. You can set any other score less than 99999. The current score is reset on game over. 1Runner.instance_.distanceRan = 12345 / Runner.instance_.distanceMeter.config.COEFFICIENT Dino jumping too high? You can control how high the dino jumps by using the below function. Adjust the value as necessary. 1Runner.instance_.tRex.setJumpVelocity(10) References Hacking the Chrome Dino Game","link":"/blog/2020/10/09/t-rex/"},{"title":"Temporal Difference Learning","text":"In this post, we will now introduce a RL method called TD learning, which can be considered as an improvement or extension of the MC-based RL approach. Similar to the MC technique, TD learning is also based on learning by experience and, therefore, does not require any knowledge of environment dynamics and transition probabilities. The main difference between the TD and MC techniques is that in MC, we have to wait until the end of the episode to be able to calculate the total return. However, in TD learning, we can leverage some of the learned properties to update the estimated values before reaching the end of the episode. This is called bootstrapping. Similar to the dynamic programming approach and MC-based learning, we will consider two tasks: estimating the value function (which is also called value prediction) and improving the policy (which is also called the control task). TD prediction Let's first revisit the value prediction by MC. At the end of each episode, we are able to estimate the return \\(G_t\\) for each time step \\(t\\). Therefore, we can update our estimates for the visited states as follows: \\[ V(S_t) = V(S_t) + \\alpha (G_t - V(S_t)) \\] Here, \\(G_t\\) is used as the target return to update the estimated values, and \\((G_t − V(S_t))\\) is a correction term added to our current estimate of the value \\(V_(S_t )\\). The value \\(\\alpha\\) is a hyperparameter denoting the learning rate, which is kept constant during learning. Notice that in MC, the correction term uses the actual return, \\(G_t\\) , which is not known until the end of the episode. To clarify this, we can rename the actual return, \\(G_t\\) , to \\(G_{t:T}\\), where the subscript \\(t:T\\) indicates that this is the return obtained at time step \\(t\\) while considering all the events occurred from time step \\(t\\) until the final time step, \\(T\\). In TD learning, we replace the actual return, \\(G_{t:T}\\), with a new target return, \\(G_{t:t+1}\\), which significantly simplifies the updates for the value function, \\(V(S_t)\\). The update-formula based on TD learning is as follows: \\[ V(S_t) = V(S_t) + \\alpha (G_{t:t+1} - V(S_t)) \\] Here, the target return, \\(G_{t:t+1} \\stackrel{\\text{def}}{=} R_{t+1} + \\gamma(S_{t+1}) = r + \\gamma(S_{t+1})\\), is using the observed reward, \\(R_{t+1} = r\\), and estimated value of the next immediate step. Notice the difference between MC and TD. In MC, \\(G_{t:T}\\) is not available until the end of the episode, so we should execute as many steps as needed to get there. On the contrary, in TD, we only need to go one step ahead to get the target return. This is also known as TD(0). Furthermore, the TD(0) algorithm can be generalized to the so-called n-step TD algorithm, which incorporates more future steps – more precisely, the weighted sum of \\(n\\) future steps. If we define \\(n = 1\\), then the \\(n\\)-step TD procedure is identical to TD(0), which was described in the previous paragraph. If \\(n \\to \\infty\\), however, the \\(n\\)-step TD algorithm will be the same as the MC algorithm. The update-rule for \\(n\\)-step TD is as follows: \\[ V(S_t) = V(S_t) + \\alpha (G_{t:t+n} - V(S_t)) \\] And \\(G_{t:t+n}\\) is defined as: \\[ G_{t:t+n} \\stackrel{\\text{def}}{=}\\left\\{\\begin{array}{ll} R_{t+1} + \\gamma R_{t+2} + \\cdots \\gamma^{n-1} R_{t+n} + \\gamma^{n} V\\left(S_{t+n}\\right) &amp; \\text{if}\\ t+n&lt;T \\\\ G_{t:T} &amp; \\text{otherwise} \\end{array}\\right. \\] MC versus TD: which method converges faster? While the precise answer to this question is still unknown, in practice, it is empirically shown that TD can converge faster than MC. If you are interested, you can find more details on the convergences of MC and TD in the book titled Reinforcement Learning: An Introduction, by Richard S. Sutton and Andrew G. Barto. Now that we have covered the prediction task using the TD algorithm, we can move on to the control task. We will cover two algorithms for TD control: an on-policy control and an off-policy control. In both cases, we use the GPI that was used in both the dynamic programming and MC algorithms. In on-policy TD control, the value function is updated based on the actions from the same policy that the agent is following, while in an off-policy algorithm, the value function is updated based on actions outside the current policy. On-policy TD control (SARSA) For simplicity, we only consider the one-step TD algorithm, or TD(0). However, the on-policy TD control algorithm can be readily generalized to \\(n\\)-step TD. We wills tart by extending the prediction formula for defining the state-value function to describe the action-value function. To do this, we use a lookup table, that is, a tabular 2D-array, \\(Q_(S_t, A_t)\\), which represents the action-value function for each state-action pair. In this case, we will have the following: \\[ Q\\left(S_{t}, A_{t}\\right)=Q\\left(S_{t}, A_{t}\\right)+\\alpha\\left[R_{t+1}+\\gamma Q\\left(S_{t+1}, A_{t+1}\\right)-Q\\left(S_{t}, A_{t}\\right)\\right] \\] This algorithm is often called SARSA, referring to the quintuple \\((S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})\\) that is used in the update formula. As we saw in the previous sections describing the dynamic programming and MC algorithms, we can use the GPI framework, and starting from the random policy, we can repeatedly estimate the action-value function for the current policy and then optimize the policy using the \\(\\epsilon\\)-greedy policy based on the current action-value function. Off-policy TD control (Q-learning) We saw when using the previous on-policy TD control algorithm that how we estimate the action-value function is based on the policy that is used in the simulated episode. After updating the action-value function, a separate step for policy improvement is performed by taking the action that has the higher value. An alternative (and better) approach is to combine these two steps. In other words, imagine the agent is following policy ππ , generating an episode with the current transition quintuple \\((S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})\\). Instead of updating the action-value function using the action value of \\(A_{t+1}\\) that is taken by the agent, we can find the best action even if it is not actually chosen by the agent following the current policy. (That's why this is considered an off-policy algorithm.) To do this, we can modify the update rule to consider the maximum Q-value by varying different actions in the next immediate state. The modified equation for updating the Q-values is as follows: \\[ Q\\left(S_{t}, A_{t}\\right)=Q\\left(S_{t}, A_{t}\\right)+\\alpha\\left[R_{t+1}+\\gamma \\max_{a} Q\\left(S_{t+1}, a\\right)-Q\\left(S_{t}, A_{t}\\right)\\right] \\] We encourage you to compare the update rule here with that of the SARSA algorithm. As you can see, we find the best action in the next state, \\(S_{t+1}\\), and use that in the correction term to update our estimate of \\(Q(S_t, A_t)\\). References S. Raschka and V. Mirjalili. Python Machine Learning: Machine Learning and Deep Learning with Python, scikit-learn, and TensorFlow 2. Packt Publishing Ltd, Birmingham, third edition, 2019.","link":"/blog/2020/05/14/temporal_difference_learning/"},{"title":"Champs Kaggle Competition: Predicting Scalar Coupling Constants","text":"[To be revised] Solution 1 -- \"hybrid\" team The \"hybrid\" team is the champion of this competition. Data Processing Features Of the provided data, they use only the element and position of each atom and the scalar coupling type of each bond. From these, additional features are generated for each atom using the open source package RDKit and Open Babel. Below lists the total set of features of each molecular object that used in the network. Atom atom index symbols: single element symbol; labeled atom symbol including element and its total number of bonds (i.e. valence), total number of bonded neighbors, largest bond order, second largest bond order; extended labeled atom symbol created by concatenating labeled atom symbol and bonded neighbor element symbols. In the code, these are stored in the atom, labeled_atom, sublabel_atom DataFrame columns, respectively. position: three-dimension Cartesian coordinates. angle with nearest neighbors: the neighbors may be not bonded to the atom. partial charge, spin, heavy valence, heterovalence, valence, hybrid type. Bond atom indices: two indices of bonded atoms distance bond order (if applicable) coupling type (if applicable): type, labeled_type, sublabel_type. scalar coupling constant predict: 1 if need to be predicted else 0. Triplet atom indices: three indices of triplet atoms labels: label, sublabel. bond angle Quadruplet atom indices: four indices of quadruplet atoms labels: label, sublabel, sublabel2, sublabel3, sublabel4. dihedral angle Tokenization Graph Transformer Atom Embedding SineEmbedding TokenFeatureEmbedding HierarchicalEmbedding embeds nn.Embedding nn.Embedding nn.Embedding projs Linear Linear 12345678910111213SineEmbedding( (embedding): HierarchicalEmbedding( (embeds): ModuleList( (0): Embedding(5, 650) (1): Embedding(6, 650) (2): Embedding(12, 650) ) ) (projs): ModuleList( (0): Linear(in_features=650, out_features=650, bias=True) (1): Linear(in_features=650, out_features=650, bias=True) )) layers 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142ModuleList( (0): GraphLayer( (dropout): Dropout(p=0.03, inplace=False) (attn_dropout): Dropout(p=0.0, inplace=False) (qkv_net): Linear(in_features=650, out_features=1950, bias=True) (o_net): Linear(in_features=650, out_features=650, bias=False) (norm1): LayerNorm((650,), eps=1e-05, elementwise_affine=True) (norm2): LayerNorm((650,), eps=1e-05, elementwise_affine=True) (proj1): Linear(in_features=650, out_features=3800, bias=True) (proj2): Linear(in_features=3800, out_features=650, bias=True) ) (1): GraphLayer( (dropout): Dropout(p=0.03, inplace=False) (attn_dropout): Dropout(p=0.0, inplace=False) (qkv_net): Linear(in_features=650, out_features=1950, bias=True) (o_net): Linear(in_features=650, out_features=650, bias=False) (norm1): LayerNorm((650,), eps=1e-05, elementwise_affine=True) (norm2): LayerNorm((650,), eps=1e-05, elementwise_affine=True) (proj1): Linear(in_features=650, out_features=3800, bias=True) (proj2): Linear(in_features=3800, out_features=650, bias=True) ) (2): GraphLayer( (dropout): Dropout(p=0.03, inplace=False) (attn_dropout): Dropout(p=0.0, inplace=False) (qkv_net): Linear(in_features=650, out_features=1950, bias=True) (o_net): Linear(in_features=650, out_features=650, bias=False) (norm1): LayerNorm((650,), eps=1e-05, elementwise_affine=True) (norm2): LayerNorm((650,), eps=1e-05, elementwise_affine=True) (proj1): Linear(in_features=650, out_features=3800, bias=True) (proj2): Linear(in_features=3800, out_features=650, bias=True) ) (3): GraphLayer( (dropout): Dropout(p=0.03, inplace=False) (attn_dropout): Dropout(p=0.0, inplace=False) (qkv_net): Linear(in_features=650, out_features=1950, bias=True) (o_net): Linear(in_features=650, out_features=650, bias=False) (norm1): LayerNorm((650,), eps=1e-05, elementwise_affine=True) (norm2): LayerNorm((650,), eps=1e-05, elementwise_affine=True) (proj1): Linear(in_features=650, out_features=3800, bias=True) (proj2): Linear(in_features=3800, out_features=650, bias=True) ) (4): GraphLayer( (dropout): Dropout(p=0.03, inplace=False) (attn_dropout): Dropout(p=0.0, inplace=False) (qkv_net): Linear(in_features=650, out_features=1950, bias=True) (o_net): Linear(in_features=650, out_features=650, bias=False) (norm1): LayerNorm((650,), eps=1e-05, elementwise_affine=True) (norm2): LayerNorm((650,), eps=1e-05, elementwise_affine=True) (proj1): Linear(in_features=650, out_features=3800, bias=True) (proj2): Linear(in_features=3800, out_features=650, bias=True) ) (5): GraphLayer( (dropout): Dropout(p=0.03, inplace=False) (attn_dropout): Dropout(p=0.0, inplace=False) (qkv_net): Linear(in_features=650, out_features=1950, bias=True) (o_net): Linear(in_features=650, out_features=650, bias=False) (norm1): LayerNorm((650,), eps=1e-05, elementwise_affine=True) (norm2): LayerNorm((650,), eps=1e-05, elementwise_affine=True) (proj1): Linear(in_features=650, out_features=3800, bias=True) (proj2): Linear(in_features=3800, out_features=650, bias=True) ) (6): GraphLayer( (dropout): Dropout(p=0.03, inplace=False) (attn_dropout): Dropout(p=0.0, inplace=False) (qkv_net): Linear(in_features=650, out_features=1950, bias=True) (o_net): Linear(in_features=650, out_features=650, bias=False) (norm1): LayerNorm((650,), eps=1e-05, elementwise_affine=True) (norm2): LayerNorm((650,), eps=1e-05, elementwise_affine=True) (proj1): Linear(in_features=650, out_features=3800, bias=True) (proj2): Linear(in_features=3800, out_features=650, bias=True) ) (7): GraphLayer( (dropout): Dropout(p=0.03, inplace=False) (attn_dropout): Dropout(p=0.0, inplace=False) (qkv_net): Linear(in_features=650, out_features=1950, bias=True) (o_net): Linear(in_features=650, out_features=650, bias=False) (norm1): LayerNorm((650,), eps=1e-05, elementwise_affine=True) (norm2): LayerNorm((650,), eps=1e-05, elementwise_affine=True) (proj1): Linear(in_features=650, out_features=3800, bias=True) (proj2): Linear(in_features=3800, out_features=650, bias=True) ) (8): GraphLayer( (dropout): Dropout(p=0.03, inplace=False) (attn_dropout): Dropout(p=0.0, inplace=False) (qkv_net): Linear(in_features=650, out_features=1950, bias=True) (o_net): Linear(in_features=650, out_features=650, bias=False) (norm1): LayerNorm((650,), eps=1e-05, elementwise_affine=True) (norm2): LayerNorm((650,), eps=1e-05, elementwise_affine=True) (proj1): Linear(in_features=650, out_features=3800, bias=True) (proj2): Linear(in_features=3800, out_features=650, bias=True) ) (9): GraphLayer( (dropout): Dropout(p=0.03, inplace=False) (attn_dropout): Dropout(p=0.0, inplace=False) (qkv_net): Linear(in_features=650, out_features=1950, bias=True) (o_net): Linear(in_features=650, out_features=650, bias=False) (norm1): LayerNorm((650,), eps=1e-05, elementwise_affine=True) (norm2): LayerNorm((650,), eps=1e-05, elementwise_affine=True) (proj1): Linear(in_features=650, out_features=3800, bias=True) (proj2): Linear(in_features=3800, out_features=650, bias=True) ) (10): GraphLayer( (dropout): Dropout(p=0.03, inplace=False) (attn_dropout): Dropout(p=0.0, inplace=False) (qkv_net): Linear(in_features=650, out_features=1950, bias=True) (o_net): Linear(in_features=650, out_features=650, bias=False) (norm1): LayerNorm((650,), eps=1e-05, elementwise_affine=True) (norm2): LayerNorm((650,), eps=1e-05, elementwise_affine=True) (proj1): Linear(in_features=650, out_features=3800, bias=True) (proj2): Linear(in_features=3800, out_features=650, bias=True) ) (11): GraphLayer( (dropout): Dropout(p=0.03, inplace=False) (attn_dropout): Dropout(p=0.0, inplace=False) (qkv_net): Linear(in_features=650, out_features=1950, bias=True) (o_net): Linear(in_features=650, out_features=650, bias=False) (norm1): LayerNorm((650,), eps=1e-05, elementwise_affine=True) (norm2): LayerNorm((650,), eps=1e-05, elementwise_affine=True) (proj1): Linear(in_features=650, out_features=3800, bias=True) (proj2): Linear(in_features=3800, out_features=650, bias=True) ) (12): GraphLayer( (dropout): Dropout(p=0.03, inplace=False) (attn_dropout): Dropout(p=0.0, inplace=False) (qkv_net): Linear(in_features=650, out_features=1950, bias=True) (o_net): Linear(in_features=650, out_features=650, bias=False) (norm1): LayerNorm((650,), eps=1e-05, elementwise_affine=True) (norm2): LayerNorm((650,), eps=1e-05, elementwise_affine=True) (proj1): Linear(in_features=650, out_features=3800, bias=True) (proj2): Linear(in_features=3800, out_features=650, bias=True) ) (13): GraphLayer( (dropout): Dropout(p=0.03, inplace=False) (attn_dropout): Dropout(p=0.0, inplace=False) (qkv_net): Linear(in_features=650, out_features=1950, bias=True) (o_net): Linear(in_features=650, out_features=650, bias=False) (norm1): LayerNorm((650,), eps=1e-05, elementwise_affine=True) (norm2): LayerNorm((650,), eps=1e-05, elementwise_affine=True) (proj1): Linear(in_features=650, out_features=3800, bias=True) (proj2): Linear(in_features=3800, out_features=650, bias=True) )) final_norm 1LayerNorm((650,), eps=1e-05, elementwise_affine=True) final_lin1 1Conv1d(650, 12600, kernel_size=(1,), stride=(1,)) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204GraphTransformer( (atom_embedding): SineEmbedding( (embedding): HierarchicalEmbedding( (embeds): ModuleList( (0): Embedding(5, 650) (1): Embedding(6, 650) (2): Embedding(12, 650) ) ) (projs): ModuleList( (0): Linear(in_features=650, out_features=650, bias=True) (1): Linear(in_features=650, out_features=650, bias=True) ) ) (bond_embedding): SineEmbedding( (embedding): HierarchicalEmbedding( (embeds): ModuleList( (0): Embedding(30, 650) (1): Embedding(46, 650) (2): Embedding(46, 650) ) ) (projs): ModuleList( (0): Linear(in_features=650, out_features=650, bias=True) ) ) (triplet_embedding): SineEmbedding( (embedding): HierarchicalEmbedding( (embeds): ModuleList( (0): Embedding(19, 650) (1): Embedding(51, 650) ) ) (projs): ModuleList( (0): Linear(in_features=650, out_features=650, bias=True) ) ) (quad_embedding): SineEmbedding( (embedding): HierarchicalEmbedding( (embeds): ModuleList( (0): Embedding(25, 650) ) ) (projs): ModuleList( (0): Linear(in_features=650, out_features=650, bias=True) ) ) (layers): ModuleList( (0): GraphLayer( (dropout): Dropout(p=0.03, inplace=False) (attn_dropout): Dropout(p=0.0, inplace=False) (qkv_net): Linear(in_features=650, out_features=1950, bias=True) (o_net): Linear(in_features=650, out_features=650, bias=False) (norm1): LayerNorm((650,), eps=1e-05, elementwise_affine=True) (norm2): LayerNorm((650,), eps=1e-05, elementwise_affine=True) (proj1): Linear(in_features=650, out_features=3800, bias=True) (proj2): Linear(in_features=3800, out_features=650, bias=True) ) (1): GraphLayer( (dropout): Dropout(p=0.03, inplace=False) (attn_dropout): Dropout(p=0.0, inplace=False) (qkv_net): Linear(in_features=650, out_features=1950, bias=True) (o_net): Linear(in_features=650, out_features=650, bias=False) (norm1): LayerNorm((650,), eps=1e-05, elementwise_affine=True) (norm2): LayerNorm((650,), eps=1e-05, elementwise_affine=True) (proj1): Linear(in_features=650, out_features=3800, bias=True) (proj2): Linear(in_features=3800, out_features=650, bias=True) ) (2): GraphLayer( (dropout): Dropout(p=0.03, inplace=False) (attn_dropout): Dropout(p=0.0, inplace=False) (qkv_net): Linear(in_features=650, out_features=1950, bias=True) (o_net): Linear(in_features=650, out_features=650, bias=False) (norm1): LayerNorm((650,), eps=1e-05, elementwise_affine=True) (norm2): LayerNorm((650,), eps=1e-05, elementwise_affine=True) (proj1): Linear(in_features=650, out_features=3800, bias=True) (proj2): Linear(in_features=3800, out_features=650, bias=True) ) (3): GraphLayer( (dropout): Dropout(p=0.03, inplace=False) (attn_dropout): Dropout(p=0.0, inplace=False) (qkv_net): Linear(in_features=650, out_features=1950, bias=True) (o_net): Linear(in_features=650, out_features=650, bias=False) (norm1): LayerNorm((650,), eps=1e-05, elementwise_affine=True) (norm2): LayerNorm((650,), eps=1e-05, elementwise_affine=True) (proj1): Linear(in_features=650, out_features=3800, bias=True) (proj2): Linear(in_features=3800, out_features=650, bias=True) ) (4): GraphLayer( (dropout): Dropout(p=0.03, inplace=False) (attn_dropout): Dropout(p=0.0, inplace=False) (qkv_net): Linear(in_features=650, out_features=1950, bias=True) (o_net): Linear(in_features=650, out_features=650, bias=False) (norm1): LayerNorm((650,), eps=1e-05, elementwise_affine=True) (norm2): LayerNorm((650,), eps=1e-05, elementwise_affine=True) (proj1): Linear(in_features=650, out_features=3800, bias=True) (proj2): Linear(in_features=3800, out_features=650, bias=True) ) (5): GraphLayer( (dropout): Dropout(p=0.03, inplace=False) (attn_dropout): Dropout(p=0.0, inplace=False) (qkv_net): Linear(in_features=650, out_features=1950, bias=True) (o_net): Linear(in_features=650, out_features=650, bias=False) (norm1): LayerNorm((650,), eps=1e-05, elementwise_affine=True) (norm2): LayerNorm((650,), eps=1e-05, elementwise_affine=True) (proj1): Linear(in_features=650, out_features=3800, bias=True) (proj2): Linear(in_features=3800, out_features=650, bias=True) ) (6): GraphLayer( (dropout): Dropout(p=0.03, inplace=False) (attn_dropout): Dropout(p=0.0, inplace=False) (qkv_net): Linear(in_features=650, out_features=1950, bias=True) (o_net): Linear(in_features=650, out_features=650, bias=False) (norm1): LayerNorm((650,), eps=1e-05, elementwise_affine=True) (norm2): LayerNorm((650,), eps=1e-05, elementwise_affine=True) (proj1): Linear(in_features=650, out_features=3800, bias=True) (proj2): Linear(in_features=3800, out_features=650, bias=True) ) (7): GraphLayer( (dropout): Dropout(p=0.03, inplace=False) (attn_dropout): Dropout(p=0.0, inplace=False) (qkv_net): Linear(in_features=650, out_features=1950, bias=True) (o_net): Linear(in_features=650, out_features=650, bias=False) (norm1): LayerNorm((650,), eps=1e-05, elementwise_affine=True) (norm2): LayerNorm((650,), eps=1e-05, elementwise_affine=True) (proj1): Linear(in_features=650, out_features=3800, bias=True) (proj2): Linear(in_features=3800, out_features=650, bias=True) ) (8): GraphLayer( (dropout): Dropout(p=0.03, inplace=False) (attn_dropout): Dropout(p=0.0, inplace=False) (qkv_net): Linear(in_features=650, out_features=1950, bias=True) (o_net): Linear(in_features=650, out_features=650, bias=False) (norm1): LayerNorm((650,), eps=1e-05, elementwise_affine=True) (norm2): LayerNorm((650,), eps=1e-05, elementwise_affine=True) (proj1): Linear(in_features=650, out_features=3800, bias=True) (proj2): Linear(in_features=3800, out_features=650, bias=True) ) (9): GraphLayer( (dropout): Dropout(p=0.03, inplace=False) (attn_dropout): Dropout(p=0.0, inplace=False) (qkv_net): Linear(in_features=650, out_features=1950, bias=True) (o_net): Linear(in_features=650, out_features=650, bias=False) (norm1): LayerNorm((650,), eps=1e-05, elementwise_affine=True) (norm2): LayerNorm((650,), eps=1e-05, elementwise_affine=True) (proj1): Linear(in_features=650, out_features=3800, bias=True) (proj2): Linear(in_features=3800, out_features=650, bias=True) ) (10): GraphLayer( (dropout): Dropout(p=0.03, inplace=False) (attn_dropout): Dropout(p=0.0, inplace=False) (qkv_net): Linear(in_features=650, out_features=1950, bias=True) (o_net): Linear(in_features=650, out_features=650, bias=False) (norm1): LayerNorm((650,), eps=1e-05, elementwise_affine=True) (norm2): LayerNorm((650,), eps=1e-05, elementwise_affine=True) (proj1): Linear(in_features=650, out_features=3800, bias=True) (proj2): Linear(in_features=3800, out_features=650, bias=True) ) (11): GraphLayer( (dropout): Dropout(p=0.03, inplace=False) (attn_dropout): Dropout(p=0.0, inplace=False) (qkv_net): Linear(in_features=650, out_features=1950, bias=True) (o_net): Linear(in_features=650, out_features=650, bias=False) (norm1): LayerNorm((650,), eps=1e-05, elementwise_affine=True) (norm2): LayerNorm((650,), eps=1e-05, elementwise_affine=True) (proj1): Linear(in_features=650, out_features=3800, bias=True) (proj2): Linear(in_features=3800, out_features=650, bias=True) ) (12): GraphLayer( (dropout): Dropout(p=0.03, inplace=False) (attn_dropout): Dropout(p=0.0, inplace=False) (qkv_net): Linear(in_features=650, out_features=1950, bias=True) (o_net): Linear(in_features=650, out_features=650, bias=False) (norm1): LayerNorm((650,), eps=1e-05, elementwise_affine=True) (norm2): LayerNorm((650,), eps=1e-05, elementwise_affine=True) (proj1): Linear(in_features=650, out_features=3800, bias=True) (proj2): Linear(in_features=3800, out_features=650, bias=True) ) (13): GraphLayer( (dropout): Dropout(p=0.03, inplace=False) (attn_dropout): Dropout(p=0.0, inplace=False) (qkv_net): Linear(in_features=650, out_features=1950, bias=True) (o_net): Linear(in_features=650, out_features=650, bias=False) (norm1): LayerNorm((650,), eps=1e-05, elementwise_affine=True) (norm2): LayerNorm((650,), eps=1e-05, elementwise_affine=True) (proj1): Linear(in_features=650, out_features=3800, bias=True) (proj2): Linear(in_features=3800, out_features=650, bias=True) ) ) (final_norm): LayerNorm((650,), eps=1e-05, elementwise_affine=True) (final_lin1): Conv1d(650, 12600, kernel_size=(1,), stride=(1,)) (final_res): Sequential( (0): ResidualBlock( (proj): Sequential( (0): Conv1d(12600, 12600, kernel_size=(1,), stride=(1,), groups=45) (1): ReLU(inplace=True) (2): Dropout(p=0.04, inplace=False) (3): Conv1d(12600, 12600, kernel_size=(1,), stride=(1,), groups=45) (4): Dropout(p=0.04, inplace=False) ) ) (1): Conv1d(12600, 45, kernel_size=(1,), stride=(1,), groups=45) )) Model Architecture Embedding sine filter embedding \\[ d_{\\text{embed}} \\] Graph Transformer Layers Defining Distances Atom As usual, the distance between two atoms \\(a\\) and \\(a^{\\prime}\\) is defined as Euclidean distance between their positions: \\[ d_{\\text{atom},\\text{atom}}(a,a^{\\prime}) = \\|z_{a} - z_{a^{\\prime}}\\|. \\] Bond \\[ d_{\\text{atom},\\text{bond}}(a,B) = \\min(d_{\\text{atom}}(a,b_{0}), d_{\\text{atom}}(a,b_{1})) \\] \\[ d_{\\text{bond},\\text{bond}}(B,B^{\\prime}) = \\frac{1}{4} (d_{\\text{atom},\\text{bond}}(b_{0},B^{\\prime}) + d_{\\text{atom},\\text{bond}}(b_{1},B^{\\prime}) + d_{\\text{atom},\\text{bond}}(b_{0}^{\\prime},B) + d_{\\text{atom},\\text{bond}}(b_{1}^{\\prime},B)) \\] Triplet \\[ d_{\\text{atom},\\text{triplet}}(a,T) = \\min_{i \\in [0,2]}(d_{\\text{atom},\\text{atom}}(a,t_{i})) + d_{\\text{atom},\\text{atom}}(a,t_{0}) \\] \\[ d_{\\text{bond},\\text{triplet}}(B,T) = \\min(d_{\\text{bond},\\text{bond}}(B,t_{01}), d_{\\text{bond},\\text{bond}}(B,t_{02})) \\] \\[ d_{\\text{triplet},\\text{triplet}}(T,T^{\\prime}) = \\frac{1}{4} (d_{\\text{bond},\\text{triplet}}(t_{01},T^{\\prime}) + d_{\\text{bond},\\text{triplet}}(t_{02},T^{\\prime}) + d_{\\text{bond},\\text{triplet}}(t_{01}^{\\prime},T) + d_{\\text{bond},\\text{triplet}}(t_{02}^{\\prime},T)) \\] Quadruplet \\[ d_{\\text{atom},\\text{quadruplet}}(a,Q) = \\min_{i \\in [0,3]}(d_{\\text{atom},\\text{atom}}(a,q_{i})) + \\min(d_{\\text{atom},\\text{atom}}(a,q_{0}), d_{\\text{atom},\\text{atom}}(a,q_{1})) \\] \\[ d_{\\text{bond},\\text{quadruplet}}(B,Q) = \\min_{ij \\in \\{01,02,13\\}}(d_{\\text{bond},\\text{bond}}(B,q_{ij})) + d_{\\text{bond},\\text{bond}}(B,q_{01}) \\] \\[ d_{\\text{triplet},\\text{quadruplet}}(T,Q) = \\min(d_{\\text{triplet},\\text{triplet}}(T,q_{012}) + d_{\\text{triplet},\\text{triplet}}(T,q_{103})) \\] \\[ d_{\\text{quadruplet},\\text{quadruplet}}(Q,Q^{\\prime}) = \\frac{1}{4} (d_{\\text{triplet},\\text{quadruplet}}(q_{012},Q^{\\prime}) + d_{\\text{triplet},\\text{quadruplet}}(q_{103},Q^{\\prime}) + d_{\\text{triplet},\\text{quadruplet}}(q_{012}^{\\prime},Q) + d_{\\text{triplet},\\text{quadruplet}}(q_{103}^{\\prime},Q)) \\]","link":"/blog/2020/10/21/champs-kaggle-competition-predicting-scalar-coupling-constants/"},{"title":"Free Energy Calculation Tutorial","text":"A common question in computational chemistry with relation to computer aided drug design is to estimate the differences in binding free energies of a series of small molecules (ligands) to a given target compound. When you first fall into the field, things seem not as easy as you think. Usually, the setup for calculation spends plenty of your time. This tutorial will take you through the way of free energy simulations in detail. Have a nice journey! Requirements Software There are some necessary software to carry out the free energy calculation, please install them correctly. GROMACS Resources: web, pkg &amp; doc. Installation: 123456789$ tar xf gromacs-2020.tar.gz$ cd gromacs-2020$ mkdir build$ cd build$ cmake .. -DGMX_BUILD_OWN_FFTW=ON -DREGRESSIONTEST_DOWNLOAD=ON$ make$ make check$ sudo make install$ ln -s ../gromacs/bin/GMXRC bin/GMXRC Issues: Set Installation Location -DCMAKE_INSTALL_PREFIX=xxx to install GROMACS to a non-standard location. CMake Error 12CMake Error at cmake/gmxManageSimd.cmake:51 (message): Cannot find AVX2 compiler flag. Use a newer compiler, or choose AVX SIMD Solution: Add -DGMX_SIMD=AVX_256 as cmake option. Offline Make 12345-- Downloading... dst='/home/leoy/Documents/Software/gromacs-2020/build/src/external/build-fftw/fftwBuild-prefix/src/fftw-3.3.8.tar.gz' timeout='none'-- Using src='http://www.fftw.org/fftw-3.3.8.tar.gz'-- Retrying... Solution: Download fftw-3.3.8.tar.gz manually, and move it to dst directory. FESetup Resources: git, pkg, doc, doi. Installation: Here shows a successful installation example. 12345678910111213141516171819202122232425262728293031323334353637$ ./FESetup1.2.1_Linux.shInstalling package FESetup release 1.2.1...Determining system environment... found Linux x86_64/64=========================== installation setup ===========================The installer will now ask for the installation directories of various MDsoftware packages and for the location of the Python 2.7 interpreter. Alloptions either have defaults (in brackets like &quot;[python]&quot;) or are optional.You can change this at any time later by editing the scriptFESetup1.2.1/FESetup64/bin/FESetup.Confirm defaults by pressing the Enter key.This installer comes with all required AmberTools built in but you canchoose to point $AMBERHOME to your own installation. It is _strongly_recommended, however, that you use the installer\\'s default as otherwiseyou would have to make several modifications to the original installation.$AMBERHOME is preset to /usr/local/amber18, do you _really_ wish to use this? [y/N]Internal AMBER installation (press Enter) or type alternative [$FES_HOME/amber16]:GROMACS appears to be installed in /usr/local/gromacs, do you wish to use this? [Y/n]NAMD directory (path contains namd2) []:DL POLY directory (path contains execute/DLPOLY.Z) []:Python 2.7 interpreter executable [python]: python2Checking if this is version 2.7... found 2.7Creating directory FESetup1.2.1Verifying archive integrity... All good.Uncompressing FESetup release 1.2.1 100%========== Installation of FESetup release 1.2.1 is now complete. ==========$ ln -s ../FESetup1.2.1/FESetup64/bin/FESetup bin/FESetup Running: Executing the script FESetup needs an input file in INI format, otherwise it will perform a default task. 1$ FESetup [infile] PLUMED Resources: web, pkg, doc. Installation: carry out the following commands to quickly install it. 1234./configure --prefix=/usr/localmake -j 4make doc # this is optional and requires proper doxygen version installedmake install And next patch the MD code. 12cd /md/root/dirplumed patch -p pymbar Resources: git, pkg, doc. Installation: choose one of the following command to install it. 1$ conda install -c omnia pymbar 1$ pip install pymbar 1$ pip install git+https://github.com/choderalab/pymbar.git Notice: In our subsequent procedures, the alchemical_analysis.py script written in Python 2 is needed, so change to Python 2 environment when install pymbar module. Getting Started Preparation First using FESetup to setup Lysozyme ligands with equilibration. In order to obtain the tutorial files please click here. We will look at the following example perturbations, i.e. benzene to o-xylene: benzene~o-xylene Let’s start by creating a tutorial directory and place the unzipped input file (key: jh0r) into that directory. 1234$ mkdir -p Free_Energy/ins/Tutorial$ cd Free_Energy/ins/Tutorial$ mv /path/to/input.zip .$ unzip input.zip The FESetup directory should have the following content: 12$ ls FESetupligands morph.in proteins setup.in We now also have a directory with ligands. The coordinates of the ligands and that of the protein need to be in the same reference frame, i.e. the ligands need to fit into the binding pocket. Using VMD this can easily be double checked. FESetup files explained The setup.in file contains again all the necessary directives to set up the protein and the two ligands as well as directives necessary for the relative free energy calculations. Let’s look at some of the parts in more detail. 12345678910111213141516171819202122232425262728293031323334353637383940[globals]logfile = T4-lysozyme.logforcefield = amber, ff14SB, tip3p, hfegaff = gaff2AFE.type = gromacsAFE.separate_vdw_elec = falsemdengine = gromacs, mdrun[ligand]basedir = ligandsfile.name = ligand.mol2box.type = rectangularbox.length = 12.0neutralize = False min.nsteps = 100molecules = benzol, o-xylenemin.nsteps = 200min.ncyc = 100min.restr_force = 10.0min.restraint = notsolvent[protein]basedir = proteinsmolecules = 181Lpropka = t[complex]# explicit enumeration of pairs, otherwise all-with-all creationpairs = 181L : benzol, 181L : o-xylenebox.type = rectangularbox.length = 12.0align_axes = Trueneutralize = Truemin.nsteps = 2000min.ncyc = 1000min.restr_force = 10.0min.restraint = :LIG We now have a section called [ligand] and a section called [protein], which contains all the information for the ligand simulation setup. basedir = ligands and file.name = ligand.mol2 tell FESetup to look in the directory ligands for filenames ligand.mol2. Looking into this directory, you find two further directories name benzol and o-xylene, which are the two molecules for which we want to compute relative binding free energies. The [ligand] section with the box information will ensure that each ligand is parametrised using the generalised amber forcefield (GAFF), neutralised and minimised. The parameter AFE.type = gromacs, sets the output to be compatible with a SOMD free energy calculation, however AMBER and Sire output formats are also supported. Respectively [protein] contains all information regarding the protein to be setup with the ligand. Running FESetup with equilibration FESetup is run in the following way: 1$ FESetup setup.in If you want to run programs on the cluster managed by Slurm which may prevent you from working on the management node, then you should apply for a computation node, log into it, and execute there. Here shows an example on the bibdr cluster: 1234$ salloc -N 1 -p GPUV100 -n 4 --gres=gpu:1 --mem=16G$ ssh gpu06$ do_something$ exit &amp;&amp; exit The output generated with FESetup should look something like this: 12345678910111213=== FESetup release 1.2.1, SUI version: 0.8.3 ===Please cite: HH Loeffler, J Michel, C Woods, J Chem Inf Mod, 55, 2485 DOI 10.1021/acs.jcim.5b00368For more information please visit http://www.ccpbiosim.ac.uk/fesetup/Making biomolecule 181L...Making ligand benzol...Making ligand o-xylene...Making complex from 181L and benzol...Making complex from 181L and o-xylene...=== All molecules built successfully === After running the above setup.in file through FESetup all the required output files are generated. The FESetup directory should now look like this: 1_complexes _ligands ligands morph.in _proteins proteins setup.in T4-lysozyme.log First of all the logfile T4-lysozyme.log contains all GROMACS Tools commands executed to parameterise and setup the ligands and protein separately and in complex. _proteins contains the solvated and minimised protein file. _ligands contains the parameterised ligands solvated in a water box and in vacuum. Up to this point we can just run box standard molecular dynamics simulations of a protein with two different ligands bound. The necessary data for this can be found in _complexes. In order to actually do an alchemical free energy calculations we need to generate a set of perturbations. This will be done with a separate morph file, called morph.in. 123456789101112131415161718192021222324FE_type = gromacsAFE.separate_vdw_elec = False[globals]forcefield = amber, ff14SB, tip3pgaff = gaff2logfile = dGmorph.log[ligand]basedir = ligandsfile.name = ligand.mol2box.type = rectangularmorph_pairs = benzol &gt; o-xylene, o-xylene &gt; benzol[protein]basedir = proteinsmolecules = 181Lpropka = t[complex]# the following are required to create the morph in solutionbox.type = rectangular FESetup is run a again as before: 1$ FESetup morph.in and the output will look something like this: 1234567891011121314=== FESetup release 1.2.1, SUI version: 0.8.3 ===Please cite: HH Loeffler, J Michel, C Woods, J Chem Inf Mod, 55, 2485 DOI 10.1021/acs.jcim.5b00368For more information please visit http://www.ccpbiosim.ac.uk/fesetup/Making biomolecule 181L...Morphs will be generated for gromacsMorphing benzol to o-xylene...Morphing o-xylene to benzol...Creating complex 181L:benzol with ligand morph benzol~o-xylene...Creating complex 181L:o-xylene with ligand morph o-xylene~benzol...=== All molecules built successfully === The output will have generated a directory called _perturbations that contains all the necessary input for running an alchemical relative free energy calculation, with perturbations ready in protein complex format as well as just the solvated ligands. FESetup output Let’s take a closer look at the _perturbations directory, that contains all necessary files for the production run. So the overall directory/file structure in the directory should now look like this: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108_perturbations/└── gromacs ├── benzol~o-xylene │ ├── complex │ │ ├── leap.log │ │ ├── ligand_removed.pdb │ │ ├── morph.gro │ │ ├── morph.top │ │ ├── pert.atp -&gt; ../pert.atp │ │ ├── pert.itp -&gt; ../pert.itp │ │ ├── _sol.mdp │ │ ├── state0.parm7 │ │ ├── state0.pdb │ │ ├── state0.rst7 │ │ ├── state1.parm7 │ │ ├── state1.pdb │ │ └── state1.rst7 │ ├── leap.log │ ├── mcs_map.pkl │ ├── mcs.mol2 │ ├── MORPH0.frcmod │ ├── MORPH0.mol2 │ ├── MORPH1.frcmod │ ├── MORPH1.mol2 │ ├── morph.gro -&gt; state0.gro │ ├── morph.top │ ├── pert.atp │ ├── pert.itp │ ├── solvated │ │ ├── leap.log │ │ ├── ligand_removed.pdb │ │ ├── morph.gro │ │ ├── morph.top │ │ ├── pert.atp -&gt; ../pert.atp │ │ ├── pert.itp -&gt; ../pert.itp │ │ ├── _sol.mdp │ │ ├── state0.parm7 │ │ ├── state0.pdb │ │ ├── state0.rst7 │ │ ├── state1.parm7 │ │ ├── state1.pdb │ │ └── state1.rst7 │ ├── state0.atp │ ├── state0.gro │ ├── state0.itp │ ├── state0.parm7 │ ├── state0.pdb │ ├── state0.rst7 │ ├── state1.atp │ ├── state1.gro │ ├── state1.itp │ ├── state1.parm7 │ ├── state1.pdb │ ├── state1.rst7 │ └── _vac.mdp └── o-xylene~benzol ├── complex │ ├── leap.log │ ├── ligand_removed.pdb │ ├── morph.gro │ ├── morph.top │ ├── pert.atp -&gt; ../pert.atp │ ├── pert.itp -&gt; ../pert.itp │ ├── _sol.mdp │ ├── state0.parm7 │ ├── state0.pdb │ ├── state0.rst7 │ ├── state1.parm7 │ ├── state1.pdb │ └── state1.rst7 ├── leap.log ├── mcs_map.pkl ├── mcs.mol2 ├── MORPH0.frcmod ├── MORPH0.mol2 ├── MORPH1.frcmod ├── MORPH1.mol2 ├── morph.gro -&gt; state0.gro ├── morph.top ├── pert.atp ├── pert.itp ├── solvated │ ├── leap.log │ ├── ligand_removed.pdb │ ├── morph.gro │ ├── morph.top │ ├── pert.atp -&gt; ../pert.atp │ ├── pert.itp -&gt; ../pert.itp │ ├── _sol.mdp │ ├── state0.parm7 │ ├── state0.pdb │ ├── state0.rst7 │ ├── state1.parm7 │ ├── state1.pdb │ └── state1.rst7 ├── state0.atp ├── state0.gro ├── state0.itp ├── state0.parm7 ├── state0.pdb ├── state0.rst7 ├── state1.atp ├── state1.gro ├── state1.itp ├── state1.parm7 ├── state1.pdb ├── state1.rst7 └── _vac.mdp With all input files generated for the alchemical free energy calculations, we can now move to the production run. Production MD File and directory structure setup Enter into _perturbations/gromacs/benzol~o-xylene/complex directory. Then create a script named fes-prep.sh, and write in 12345678910111213141516#!/bin/bash# FES Preparationgro=&quot;morph.gro&quot;top=&quot;morph.top&quot;base=&quot;_sol&quot; nstates=11mkdir prodfor (( i = 0; i &lt; nstates; i++ ))do imdp=&quot;$base.$i.mdp&quot; itpr=&quot;$base.$i.tpr&quot; sed 's/%L%/'&quot;$i&quot;'/' $base.mdp &gt; $imdp gmx grompp -f $imdp -c em.gro -p $top -o prod/$itprdone Now, a folder prod is created, in which resides eleven .tpr files. Production runs Under the PBS job scheduling environment, you can use the following job0.pbs script to run a simulation for the complex. And others are run in the similar way. 1234567891011#PBS -q mix#PBS -V#PBS -S /bin/bash#PBS -j oe#PBS -N samp#PBS -l nodes=1:ppn=1#PBS -l walltime=1024:00:00echo &quot;begin: $(date +&quot;%D %T&quot;)&quot;gmx mdrun -deffnm outputs/_sol.0 -dhdl outputs/_sol.0.dhdl.xvgecho &quot;end: $(date +&quot;%D %T&quot;)&quot; In the Slurm job scheduling system, you can use job0.slurm script like this 123456789101112131415#!/bin/bash -l#SBATCH --job-name=samp#SBATCH --partition all#SBATCH --nodes 1#SBATCH --gpus=1#SBATCH --cpus-per-gpu=6#SBATCH --mem=60Gmodule load openmpi/3.1.4module load cuda/10.0module load cuDNN/v7.6forcuda10.0module load plumed/2.5.1module load gromacs/2018.3gmx mdrun -deffnm prod/_sol.0 -dhdl prod/_sol.0.dhdl.xvg The same procedures should also apply to the case of ligand in water, namely files locate at the _perturbations/gromacs/benzol~o-xylene/solvated path. After MD simulation, a series of *.dhdl.xvg files come out, which will be used to compute the free energy difference in the next step. Analysis Thermodynamic cycles In order to compute a free energy of binding we make use of thermodynamic cycles. Thermodynamic Cycle Thermodynamic cycle for the relative free energy of binding for benzene and xylene. The free energy of binding can be computed as the free energy difference of the alchemical transformation of the ligand bound to the protein and the ligand in solution, i.e. water. \\[ \\Delta \\Delta G_{\\mathrm{bind}}=\\Delta G_{\\mathrm{bound}}-\\Delta G_{\\mathrm{sol}} \\] The individual terms can be computed in different ways. Two common approaches are thermodynamic integration and multi state Bennet’s acceptance ration (MBAR). Here we use the MBAR method provided by pymbar. Computing a free energy difference with pymbar Copy the _sol.$i.dhdl.xvg files to dhdl.$i.xvg: 12$ mkdir anal$ for i in `seq 0 10`; do cp prod/_sol.$i.dhdl.xvg mkdir anal/dhdl.$i.xvg; done Then invoke the alchemical_analysis.py script to analysis these MD results (using env-py2): 1$ alchemical_analysis -d anal It will print the analysis results on screen and also save it to a results.txt file. The contents looks like this: 1234567891011121314151617------------ --------------------- ... --------------------- --------------------- States TI (kJ/mol) ... BAR (kJ/mol) MBAR (kJ/mol)------------ --------------------- ... --------------------- --------------------- 0 -- 1 0.339 +- 0.049 ... 0.204 +- 0.040 1.947 +- 0.030 1 -- 2 -2.071 +- 0.043 ... -2.149 +- 0.032 -0.360 +- 0.018 2 -- 3 -2.935 +- 0.043 ... -2.995 +- 0.030 -1.599 +- 0.014 3 -- 4 -3.272 +- 0.046 ... -3.315 +- 0.029 -2.357 +- 0.013 4 -- 5 -3.307 +- 0.050 ... -3.335 +- 0.030 -2.738 +- 0.013 5 -- 6 -3.012 +- 0.054 ... -3.085 +- 0.030 -2.699 +- 0.015 6 -- 7 -2.183 +- 0.059 ... -2.259 +- 0.031 -2.121 +- 0.017 7 -- 8 -0.588 +- 0.064 ... -0.597 +- 0.033 -0.740 +- 0.022 8 -- 9 1.666 +- 0.074 ... 1.606 +- 0.041 1.892 +- 0.034 9 -- 10 3.890 +- 0.109 ... 4.019 +- 0.053 3.965 +- 0.055------------ --------------------- ... --------------------- --------------------- Coulomb: -11.474 +- 0.230 ... -11.906 +- 0.112 -4.811 +- 0.110 vdWaals: 0.000 +- 0.120 ... 0.000 +- 0.000 0.000 +- 0.000 TOTAL: -11.474 +- 0.259 ... -11.906 +- 0.112 -4.811 +- 0.110 References https://siremol.org/tutorials/somd/Binding_free_energy/FESetup.html https://github.com/chryswoods/siremol.org","link":"/blog/2020/01/07/free-energy-calculation-tutorial/"}],"tags":[{"name":"Neural Network","slug":"Neural-Network","link":"/blog/tags/Neural-Network/"},{"name":"Activation Function","slug":"Activation-Function","link":"/blog/tags/Activation-Function/"},{"name":"Tripos","slug":"Tripos","link":"/blog/tags/Tripos/"},{"name":"Mol2","slug":"Mol2","link":"/blog/tags/Mol2/"},{"name":"Attention Mechanism","slug":"Attention-Mechanism","link":"/blog/tags/Attention-Mechanism/"},{"name":"Free Energy","slug":"Free-Energy","link":"/blog/tags/Free-Energy/"},{"name":"Perturbation Theory","slug":"Perturbation-Theory","link":"/blog/tags/Perturbation-Theory/"},{"name":"Retrosynthesis","slug":"Retrosynthesis","link":"/blog/tags/Retrosynthesis/"},{"name":"Route Planning","slug":"Route-Planning","link":"/blog/tags/Route-Planning/"},{"name":"Conda","slug":"Conda","link":"/blog/tags/Conda/"},{"name":"Python","slug":"Python","link":"/blog/tags/Python/"},{"name":"Workflow","slug":"Workflow","link":"/blog/tags/Workflow/"},{"name":"BibTeX","slug":"BibTeX","link":"/blog/tags/BibTeX/"},{"name":"DOI","slug":"DOI","link":"/blog/tags/DOI/"},{"name":"arXiv","slug":"arXiv","link":"/blog/tags/arXiv/"},{"name":"ISBN","slug":"ISBN","link":"/blog/tags/ISBN/"},{"name":"Ubuntu","slug":"Ubuntu","link":"/blog/tags/Ubuntu/"},{"name":"SSH","slug":"SSH","link":"/blog/tags/SSH/"},{"name":"R","slug":"R","link":"/blog/tags/R/"},{"name":"Graph","slug":"Graph","link":"/blog/tags/Graph/"},{"name":"Loss Function","slug":"Loss-Function","link":"/blog/tags/Loss-Function/"},{"name":"Scoring Function","slug":"Scoring-Function","link":"/blog/tags/Scoring-Function/"},{"name":"Protein−Ligand Complex","slug":"Protein−Ligand-Complex","link":"/blog/tags/Protein%E2%88%92Ligand-Complex/"},{"name":"Binding Affinity","slug":"Binding-Affinity","link":"/blog/tags/Binding-Affinity/"},{"name":"Matplotlib","slug":"Matplotlib","link":"/blog/tags/Matplotlib/"},{"name":"Scalar Coupling","slug":"Scalar-Coupling","link":"/blog/tags/Scalar-Coupling/"},{"name":"Music Score","slug":"Music-Score","link":"/blog/tags/Music-Score/"},{"name":"C++ Thread Library","slug":"C-Thread-Library","link":"/blog/tags/C-Thread-Library/"},{"name":"Message Passing Interface","slug":"Message-Passing-Interface","link":"/blog/tags/Message-Passing-Interface/"},{"name":"Machine Learning","slug":"Machine-Learning","link":"/blog/tags/Machine-Learning/"},{"name":"Structure Preparation","slug":"Structure-Preparation","link":"/blog/tags/Structure-Preparation/"},{"name":"RDKit","slug":"RDKit","link":"/blog/tags/RDKit/"},{"name":"Chrome","slug":"Chrome","link":"/blog/tags/Chrome/"},{"name":"Dino","slug":"Dino","link":"/blog/tags/Dino/"},{"name":"Reinforcement Learning","slug":"Reinforcement-Learning","link":"/blog/tags/Reinforcement-Learning/"},{"name":"Q-Learning","slug":"Q-Learning","link":"/blog/tags/Q-Learning/"},{"name":"Tutorial","slug":"Tutorial","link":"/blog/tags/Tutorial/"}],"categories":[{"name":"Machine Learning","slug":"Machine-Learning","link":"/blog/categories/Machine-Learning/"},{"name":"Chemoinformatics","slug":"Chemoinformatics","link":"/blog/categories/Chemoinformatics/"},{"name":"Molecular Dynamics","slug":"Molecular-Dynamics","link":"/blog/categories/Molecular-Dynamics/"},{"name":"Synthesis Planning","slug":"Synthesis-Planning","link":"/blog/categories/Synthesis-Planning/"},{"name":"Python","slug":"Python","link":"/blog/categories/Python/"},{"name":"Conventions","slug":"Chemoinformatics/Conventions","link":"/blog/categories/Chemoinformatics/Conventions/"},{"name":"LaTeX","slug":"LaTeX","link":"/blog/categories/LaTeX/"},{"name":"Linux","slug":"Linux","link":"/blog/categories/Linux/"},{"name":"Free Energy Calculation","slug":"Molecular-Dynamics/Free-Energy-Calculation","link":"/blog/categories/Molecular-Dynamics/Free-Energy-Calculation/"},{"name":"Graph Neural Networks","slug":"Machine-Learning/Graph-Neural-Networks","link":"/blog/categories/Machine-Learning/Graph-Neural-Networks/"},{"name":"Molecular Modeling","slug":"Molecular-Modeling","link":"/blog/categories/Molecular-Modeling/"},{"name":"Music","slug":"Music","link":"/blog/categories/Music/"},{"name":"Parallel Programming","slug":"Parallel-Programming","link":"/blog/categories/Parallel-Programming/"},{"name":"Game","slug":"Game","link":"/blog/categories/Game/"},{"name":"Reinforcement Learning","slug":"Machine-Learning/Reinforcement-Learning","link":"/blog/categories/Machine-Learning/Reinforcement-Learning/"},{"name":"Scoring Function","slug":"Molecular-Modeling/Scoring-Function","link":"/blog/categories/Molecular-Modeling/Scoring-Function/"},{"name":"Molecular Property Prediction","slug":"Molecular-Modeling/Molecular-Property-Prediction","link":"/blog/categories/Molecular-Modeling/Molecular-Property-Prediction/"}]}